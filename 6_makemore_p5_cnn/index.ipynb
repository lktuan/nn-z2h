{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"NN-Z2H Lesson 6: Building makemore part 5 - Building a `WaveNet`\"\n",
        "description: \"CNN/`WaveNet` and a lot of reading of documentation, keeping track of multidimensional tensor shapes, moving between jupyter notebooks and repository code, ...\"\n",
        "author:\n",
        "  - name: \"Tuan Le Khac\"\n",
        "    url: https://lktuan.github.io/\n",
        "categories: [til, python, andrej karpathy, nn-z2h, neural networks] \n",
        "date: 12-09-2024\n",
        "date-modified: 12-09-2024\n",
        "image: wavenet.png\n",
        "draft: false\n",
        "format:\n",
        "  html:\n",
        "    code-overflow: wrap\n",
        "    code-tools: true\n",
        "    code-fold: show\n",
        "    code-annotations: hover\n",
        "execute:\n",
        "  eval: false\n",
        "---"
      ],
      "id": "e4497ba3"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "::: {.callout-important title=\"This is not orginal content!\"}\n",
        "This is my study notes / codes along with Andrej Karpathy's \"[Neural Networks: Zero to Hero](https://www.youtube.com/watch?v=VMj-3S1tku0&list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ)\" series.\n",
        ":::\n",
        "\n",
        "*Codes are executed in Colab, this calculation capacity exceeds my computer's ability.*\n",
        "\n",
        "# 1 intro\n",
        "\n",
        "We are going to take the 2-layer MLP in the part 3 of `makemore` and complexify it by:\n",
        "\n",
        "-   extending the block size: from 3 to 8 characters;\n",
        "-   making it deeper rather than 1 hidden layer.\n",
        "\n",
        "then end of with a Convoluntional Neural Network architecture similar to `WaveNet` (2016) by [Google DeepMind](https://deepmind.google/).\n",
        "\n",
        "![WaveNet model architecture, [source](https://www.researchgate.net/figure/WaveNet-Model-Architecture-38_fig2_380566531)](wavenet.png)\n",
        "\n",
        "## starter code walk through\n",
        "\n",
        "#### import libraries"
      ],
      "id": "d54661a2"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "id": "d42872b0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### reading data"
      ],
      "id": "419f1397"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import pandas as pd\n",
        "\n",
        "url = \"https://raw.githubusercontent.com/karpathy/makemore/refs/heads/master/names.txt\"\n",
        "words = pd.read_csv(url, header=None).iloc[:, 0].tolist()\n",
        "words[:8]\n",
        "\n",
        "# >>> ['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia']"
      ],
      "id": "deaf6b86",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### building vocab"
      ],
      "id": "3acc049d"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# build the vocabulary of characters and mapping to/from integer\n",
        "chars = sorted(list(set(''.join(words))))\n",
        "stoi = {s:i+1 for i, s in enumerate(chars)}\n",
        "stoi['.'] = 0\n",
        "itos = {i: s for s, i in stoi.items()}\n",
        "vocab_size = len(itos)\n",
        "print(itos)\n",
        "print(vocab_size)\n",
        "\n",
        "# itos: {1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', \n",
        "# 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', \n",
        "# 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '.'}\n",
        "# vocab_size: 27"
      ],
      "id": "adead905",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### initializing randomization"
      ],
      "id": "ba0b1ec1"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import random\n",
        "random.seed(42)\n",
        "random.shuffle(words)"
      ],
      "id": "9579f58a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### create train/dev/test splits"
      ],
      "id": "70aeb5c3"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "block_size = 3 # context length: how many characters do we take to predict the next one?\n",
        "# build the dataset\n",
        "def buid_dataset(words):\n",
        "    X, Y = [], []\n",
        "\n",
        "    for w in words:\n",
        "        context = [0] * block_size\n",
        "        for ch in w + '.':\n",
        "            ix = stoi[ch]\n",
        "            X.append(context)\n",
        "            Y.append(ix)\n",
        "            context = context[1:] + [ix]\n",
        "\n",
        "    X = torch.tensor(X)\n",
        "    Y = torch.tensor(Y)\n",
        "    print(X.shape, Y.shape)\n",
        "    return X, Y\n",
        "\n",
        "n1 = int(0.8 * len(words))\n",
        "n2 = int(0.9 * len(words))\n",
        "\n",
        "Xtr, Ytr = buid_dataset(words[:n1])        # 80#\n",
        "Xdev, Ydev = buid_dataset(words[n1:n2])    # 10%\n",
        "Xte, Yte = buid_dataset(words[n2:])        # 10%\n",
        "\n",
        "# torch.Size([182625, 3]) torch.Size([182625])\n",
        "# torch.Size([22655, 3]) torch.Size([22655])\n",
        "# torch.Size([22866, 3]) torch.Size([22866])"
      ],
      "id": "226c5c8e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### input and response preview"
      ],
      "id": "b518c376"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "for x, y in zip(Xtr[:20], Ytr[:20]):\n",
        "  print(''.join(itos[ix.item()] for ix in x), '--->', itos[y.item()])"
      ],
      "id": "ff7fa5e3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "``` md\n",
        "... ---> y\n",
        "..y ---> u\n",
        ".yu ---> h\n",
        "yuh ---> e\n",
        "uhe ---> n\n",
        "hen ---> g\n",
        "eng ---> .\n",
        "... ---> d\n",
        "..d ---> i\n",
        ".di ---> o\n",
        "dio ---> n\n",
        "ion ---> d\n",
        "ond ---> r\n",
        "ndr ---> e\n",
        "dre ---> .\n",
        "... ---> x\n",
        "..x ---> a\n",
        ".xa ---> v\n",
        "xav ---> i\n",
        "avi ---> e\n",
        "```\n",
        "\n",
        "#### initializing objects in networks\n",
        "\n",
        "Near copy paste of the layers we have developed in Part 3, I added some docstring to the classes.\n",
        "\n",
        "##### class `Linear`"
      ],
      "id": "04398d97"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| code-fold: true\n",
        "class Linear:\n",
        "  \"\"\"    \n",
        "  Applies an affine linear transformation to the incoming data: y = xA^T + b.\n",
        "\n",
        "  This class implements a linear (fully connected) layer, which performs a linear \n",
        "  transformation on the input tensor. It is typically used in neural network architectures \n",
        "  to transform input features between layers.\n",
        "\n",
        "  Args:\n",
        "      fan_in (int): Number of input features (input dimension).\n",
        "      fan_out (int): Number of output features (output dimension).\n",
        "      bias (bool, optional): Whether to include a learnable bias term. \n",
        "          Defaults to True.\n",
        "\n",
        "  Attributes:\n",
        "      weight (torch.Tensor): Weight matrix of shape (fan_in, fan_out), \n",
        "          initialized using Kaiming initialization.\n",
        "      bias (torch.Tensor or None): Bias vector of shape (fan_out), \n",
        "          initialized to zeros if bias is True, otherwise None.\n",
        "\n",
        "  Methods:\n",
        "      __call__(x): Applies the linear transformation to the input tensor x.\n",
        "      parameters(): Returns a list of trainable parameters (weight and bias).\n",
        "\n",
        "  Example:\n",
        "      >>> layer = Linear(10, 5)  # Creates a linear layer with 10 input features and 5 output features\n",
        "      >>> x = torch.randn(3, 10)  # Input tensor with batch size 3 and 10 features\n",
        "      >>> output = layer(x)  # Applies linear transformation\n",
        "      >>> output.shape\n",
        "      torch.Size([3, 5])\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, fan_in, fan_out, bias=True):\n",
        "    self.weight = torch.randn((fan_in, fan_out)) / fan_in**0.5 # note: kaiming init\n",
        "    self.bias = torch.zeros(fan_out) if bias else None\n",
        "  \n",
        "  def __call__(self, x):\n",
        "    self.out = x @ self.weight\n",
        "    if self.bias is not None:\n",
        "      self.out += self.bias\n",
        "    return self.out\n",
        "  \n",
        "  def parameters(self):\n",
        "    return [self.weight] + ([] if self.bias is None else [self.bias])"
      ],
      "id": "d571dbce",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### class `BatchNorm1d`"
      ],
      "id": "53ffcb81"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| code-fold: true\n",
        "class BatchNorm1d:\n",
        "  \"\"\"\n",
        "  Applies Batch Normalization to the input tensor, a technique to improve \n",
        "  training stability and performance in deep neural networks.\n",
        "\n",
        "  Batch Normalization normalizes the input across the batch dimension, \n",
        "  reducing internal covariate shift and allowing higher learning rates. \n",
        "  This implementation supports both training and inference modes.\n",
        "\n",
        "  Args:\n",
        "      dim (int): Number of features or channels to be normalized.\n",
        "      eps (float, optional): A small constant added to the denominator for \n",
        "          numerical stability to prevent division by zero. \n",
        "          Defaults to 1e-5.\n",
        "      momentum (float, optional): Momentum for updating running mean and \n",
        "          variance during training. Controls the degree of exponential \n",
        "          moving average. Defaults to 0.1.\n",
        "\n",
        "  Attributes:\n",
        "      eps (float): Epsilon value for numerical stability.\n",
        "      momentum (float): Momentum for running statistics update.\n",
        "      training (bool): Indicates whether the layer is in training or inference mode.\n",
        "      gamma (torch.Tensor): Learnable scale parameter of shape (dim,).\n",
        "      beta (torch.Tensor): Learnable shift parameter of shape (dim,).\n",
        "      running_mean (torch.Tensor): Exponential moving average of batch means.\n",
        "      running_var (torch.Tensor): Exponential moving average of batch variances.\n",
        "\n",
        "  Methods:\n",
        "      __call__(x): Applies batch normalization to the input tensor.\n",
        "      parameters(): Returns learnable parameters (gamma and beta).\n",
        "\n",
        "  Key Normalization Steps:\n",
        "  1. Compute batch mean and variance (in training mode)\n",
        "  2. Normalize input by subtracting mean and dividing by standard deviation\n",
        "  3. Apply learnable scale (gamma) and shift (beta) parameters\n",
        "  4. Update running statistics during training\n",
        "\n",
        "  Example:\n",
        "      >>> batch_norm = BatchNorm1d(64)  # For 64-channel input\n",
        "      >>> x = torch.randn(32, 64)  # Batch of 32 samples with 64 features\n",
        "      >>> normalized_x = batch_norm(x)  # Apply batch normalization\n",
        "      >>> normalized_x.shape\n",
        "      torch.Size([32, 64])\n",
        "\n",
        "  Note:\n",
        "      - Supports both 2D (batch, features) and 3D (batch, channels, sequence) input tensors\n",
        "      - During inference, uses running statistics instead of batch statistics\n",
        "  \"\"\"\n",
        "  def __init__(self, dim, eps=1e-5, momentum=0.1):\n",
        "    self.eps = eps\n",
        "    self.momentum = momentum\n",
        "    self.training = True\n",
        "    # parameters (trained with backprop)\n",
        "    self.gamma = torch.ones(dim)\n",
        "    self.beta = torch.zeros(dim)\n",
        "    # buffers (trained with a running 'momentum update')\n",
        "    self.running_mean = torch.zeros(dim)\n",
        "    self.running_var = torch.ones(dim)\n",
        "  \n",
        "  def __call__(self, x):\n",
        "    # calculate the forward pass\n",
        "    if self.training:\n",
        "      xmean = x.mean(dim, keepdim=True) # batch mean\n",
        "      xvar = x.var(dim, keepdim=True) # batch variance\n",
        "    else:\n",
        "      xmean = self.running_mean\n",
        "      xvar = self.running_var\n",
        "    xhat = (x - xmean) / torch.sqrt(xvar + self.eps) # normalize to unit variance\n",
        "    self.out = self.gamma * xhat + self.beta\n",
        "    # update the buffers\n",
        "    if self.training:\n",
        "      with torch.no_grad():\n",
        "        self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * xmean\n",
        "        self.running_var = (1 - self.momentum) * self.running_var + self.momentum * xvar\n",
        "    return self.out\n",
        "  \n",
        "  def parameters(self):\n",
        "    return [self.gamma, self.beta]"
      ],
      "id": "8b04979c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### class `Tanh`"
      ],
      "id": "8a58222a"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| code-fold: true\n",
        "class Tanh:\n",
        "    \"\"\"\n",
        "    Hyperbolic Tangent (Tanh) Activation Function\n",
        "\n",
        "    Applies the hyperbolic tangent activation function element-wise to the input tensor. \n",
        "    Tanh maps input values to the range [-1, 1], providing a symmetric and non-linear \n",
        "    transformation that helps neural networks learn complex patterns.\n",
        "\n",
        "    Mathematical Definition:\n",
        "    tanh(x) = (e^x - e^-x) / (e^x + e^-x)\n",
        "    \n",
        "    Key Characteristics:\n",
        "    - Output Range: [-1, 1]\n",
        "    - Symmetric around the origin\n",
        "    - Gradient is always less than 1, which helps mitigate the vanishing gradient problem\n",
        "    - Commonly used in recurrent neural networks and hidden layers\n",
        "\n",
        "    Methods:\n",
        "        __call__(x): Applies the Tanh activation to the input tensor.\n",
        "        parameters(): Returns an empty list, as Tanh has no learnable parameters.\n",
        "\n",
        "    Attributes:\n",
        "        out (torch.Tensor): Stores the output of the most recent forward pass.\n",
        "\n",
        "    Example:\n",
        "        >>> activation = Tanh()\n",
        "        >>> x = torch.tensor([-2.0, 0.0, 2.0])\n",
        "        >>> y = activation(x)\n",
        "        >>> y\n",
        "        tensor([-0.9640, 0.0000, 0.9640])\n",
        "\n",
        "    Note:\n",
        "        This implementation is stateless and does not modify the input tensor.\n",
        "        The activation is applied element-wise, preserving the input tensor's shape.\n",
        "    \"\"\"\n",
        "\n",
        "    def __call__(self, x):\n",
        "        self.out = torch.tanh(x)\n",
        "        return self.out\n",
        "\n",
        "    def parameters(self):\n",
        "        return []"
      ],
      "id": "13974901",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### random number generator"
      ],
      "id": "cc46200c"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "torch.manual_seed(42); # seed rng for reproducibility"
      ],
      "id": "30986435",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### network architecture"
      ],
      "id": "1f8bdd0f"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| eval: false\n",
        "# original network\n",
        "n_embd = 10 # the dimensionality of the character embedding vectors\n",
        "n_hidden = 200 # the number of neurons in the hidden layer of the MLP\n",
        "\n",
        "C = torch.rand((vocab_size, n_embd))\n",
        "layers = [\n",
        "  Linear(n_embd * block_size, n_hidden, bias=False),\n",
        "  BatchNorm1d(n_hidden),\n",
        "  Tanh(),\n",
        "  Linear(n_hidden, vocab_size),\n",
        "]\n",
        "\n",
        "# parameter init\n",
        "with torch.no_grad():\n",
        "  layers[-1].weight *= 0.1 # last layer make less confident\n",
        "\n",
        "parameters = [C] + [p for layer in layers for p in layer.parameters()]\n",
        "print(sum(p.nelement() for p in parameters)) # number of parameters in total\n",
        "for p in parameters:\n",
        "  p.requires_grad = True\n",
        "\n",
        "# model params: 12097"
      ],
      "id": "f4e542a7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### optimization"
      ],
      "id": "440e10c0"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| eval: false\n",
        "# same optimization as last time\n",
        "max_steps = 200000\n",
        "batch_size = 32\n",
        "lossi = []\n",
        "\n",
        "for i in range(max_steps):\n",
        "  \n",
        "  # minibatch construct\n",
        "  ix = torch.randint(0, Xtr.shape[0], (batch_size,))\n",
        "  Xb, Yb = Xtr[ix], Ytr[ix] # batch X,Y\n",
        "  \n",
        "  # forward pass\n",
        "  emb = C[Xb] # embed the characters into vectors   \n",
        "  x = emb.view(emb.shape[0], -1) # concatenate the vectors\n",
        "  for layer in layers:\n",
        "    x = layer(x)\n",
        "  loss = F.cross_entropy(x, Yb) # loss function\n",
        "  \n",
        "  # backward pass\n",
        "  for p in parameters:\n",
        "    p.grad = None\n",
        "  loss.backward()\n",
        "  \n",
        "  # update: simple SGD\n",
        "  lr = 0.1 if i < 150000 else 0.01 # step learning rate decay\n",
        "  for p in parameters:\n",
        "    p.data += -lr * p.grad\n",
        "\n",
        "  # track stats\n",
        "  if i % 10000 == 0: # print every once in a while\n",
        "    print(f'{i:7d}/{max_steps:7d}: {loss.item():.4f}')\n",
        "  lossi.append(loss.log10().item())"
      ],
      "id": "b2798876",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "``` md\n",
        "     0/ 200000: 3.2885\n",
        "  10000/ 200000: 2.3938\n",
        "  20000/ 200000: 2.1235\n",
        "  30000/ 200000: 1.9222\n",
        "  40000/ 200000: 2.2440\n",
        "  50000/ 200000: 2.1108\n",
        "  60000/ 200000: 2.0624\n",
        "  70000/ 200000: 2.0893\n",
        "  80000/ 200000: 2.4173\n",
        "  90000/ 200000: 1.9744\n",
        " 100000/ 200000: 2.0883\n",
        " 110000/ 200000: 2.4538\n",
        " 120000/ 200000: 1.9535\n",
        " 130000/ 200000: 1.8980\n",
        " 140000/ 200000: 2.1196\n",
        " 150000/ 200000: 2.3550\n",
        " 160000/ 200000: 2.2957\n",
        " 170000/ 200000: 2.0286\n",
        " 180000/ 200000: 2.2379\n",
        " 190000/ 200000: 2.3866\n",
        "```\n",
        "\n",
        "#### observe training process/evaluation"
      ],
      "id": "c704815b"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "plt.plot(lossi)"
      ],
      "id": "9860d985",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![`lossi` plot at the beginning](1_pre_lossi.png)\n",
        "\n",
        "#### calibrate the `batchnorm` after training\n",
        "\n",
        "We should be using the running mean/variance of the whole dataset splits rather than the last mini-batch."
      ],
      "id": "8c5b5999"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# put layers into eval mode (needed for batchnorm especially)\n",
        "for layer in model.layers:\n",
        "  layer.training = False"
      ],
      "id": "fcc5824d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### calculate on whole training and validation splits"
      ],
      "id": "5196118d"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# evaluate the loss\n",
        "@torch.no_grad() # this decorator disables gradient tracking inside pytorch\n",
        "def split_loss(split):\n",
        "  x,y = {\n",
        "    'train': (Xtr, Ytr),\n",
        "    'val': (Xdev, Ydev),\n",
        "    'test': (Xte, Yte),\n",
        "  }[split]\n",
        "  logits = model(x)\n",
        "  loss = F.cross_entropy(logits, y)\n",
        "  print(split, loss.item())\n",
        "\n",
        "split_loss('train')\n",
        "split_loss('val')"
      ],
      "id": "8a4b720c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Pretty loss but there are still room for improve:\n",
        "\n",
        "``` md\n",
        "train 2.0467958450317383\n",
        "val 2.0989298820495605\n",
        "```\n",
        "\n",
        "#### sample from the model\n",
        "\n",
        "Here are Names generated by the model till now, we have relatively name-like results that do not exist in the training set.\n",
        "\n",
        "``` md\n",
        "liz.\n",
        "layah.\n",
        "dan.\n",
        "hilon.\n",
        "avani.\n",
        "korron.\n",
        "aua.\n",
        "noon.\n",
        "bethalyn.\n",
        "thia.\n",
        "bote.\n",
        "jereanail.\n",
        "vitorien.\n",
        "zarashivonna.\n",
        "yakurrren.\n",
        "jovon.\n",
        "malynn.\n",
        "vanna.\n",
        "caparmana.\n",
        "shantymonse.\n",
        "```\n",
        "\n",
        "## let’s fix the learning rate plot\n",
        "\n",
        "The plot for `lossi` looks very crazy, it's because the batch size of 32 is way too few so this time we got lucky, and next time we got unlucky. And the mini-batch loss splashed too much. We should probably fix it.\n",
        "\n",
        "We pivot to a row for every 1000 observations of `lossi` and calculate the mean, we end up have 200 observations which is easier to see."
      ],
      "id": "ed3d6fcd"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "plt.plot(torch.tensor(lossi).view(-1, 1000).mean(1))"
      ],
      "id": "44a1e208",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can also observe the learning rate decay at 150k training loops.\n",
        "\n",
        "![`lossi` plot fixed](2_enhance_lossi.png)\n",
        "\n",
        "## pytorchifying our code: layers, containers, `torch.nn`, fun bugs\n",
        "\n",
        "Now we notice that we still have the embedding operation lying outside the pytorch-ified layers. It basically creating a lookup table `C`, embedding it with our data `Y` (or `Yb`), then stretching out to row with `view()` which is very cheap in PyTorch as no more memory creation is needed.\n",
        "\n",
        "We modulize this by constructing 2 classes:"
      ],
      "id": "5c806f7c"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "class Embedding:\n",
        "  \n",
        "  def __init__(self, num_embeddings, embedding_dim):\n",
        "    self.weight = torch.randn((num_embeddings, embedding_dim))\n",
        "    \n",
        "  def __call__(self, IX):\n",
        "    self.out = self.weight[IX]\n",
        "    return self.out\n",
        "  \n",
        "  def parameters(self):\n",
        "    return [self.weight]\n",
        "\n",
        "# -----------------------------------------------------------------------------------------------\n",
        "class Flatten:\n",
        "\n",
        "  def __call__(self, x):\n",
        "    self.out = x.view(x.shape[0], -1)\n",
        "    return self.out\n",
        "  \n",
        "  def parameters(self):\n",
        "    return []"
      ],
      "id": "cb39c805",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we can re-define the `layers` like this:"
      ],
      "id": "672f4e89"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "layers = [\n",
        "  Embedding(vocab_size, n_embd),\n",
        "  Flatten(),\n",
        "  Linear(n_embd * block_size, n_hidden, bias=False),\n",
        "  BatchNorm1d(n_hidden),\n",
        "  Tanh(),\n",
        "  Linear(n_hidden, vocab_size),\n",
        "]"
      ],
      "id": "b602adb0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "and also remove the `C`, `emb` definition in the forward pass construction. Going futher, we will be not only pytorchifying the elements of `layers` only, but also the `layers` itself. In PyTorch, we have term `containers`, which specifying how we organize the layers in a network. And what are we doing here is constructing layers sequentially, which is equivalent to `Sequential` in the `containers`:"
      ],
      "id": "d3516bec"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "class Sequential:\n",
        "  \n",
        "  def __init__(self, layers):\n",
        "    self.layers = layers\n",
        "  \n",
        "  def __call__(self, x):\n",
        "    for layer in self.layers:\n",
        "      x = layer(x)\n",
        "    self.out = x\n",
        "    return self.out\n",
        "  \n",
        "  def parameters(self):\n",
        "    # get parameters of all layers and stretch them out into one list\n",
        "    return [p for layer in self.layers for p in layer.parameters()]"
      ],
      "id": "20a3d9a4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "and wrapp the layers into our `model`:"
      ],
      "id": "5dd61e78"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "model = Sequential([\n",
        "  Embedding(vocab_size, n_embd),\n",
        "  Flatten(),\n",
        "  Linear(n_embd * block_size, n_hidden, bias=False),\n",
        "  BatchNorm1d(n_hidden),\n",
        "  Tanh(),\n",
        "  Linear(n_hidden, vocab_size),\n",
        "])"
      ],
      "id": "416036dd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 2 implementing `WaveNet`\n",
        "\n",
        "So far with the classical MLP following Bengio et al. (2003), we have a *embedding layer* followed by a *hidden layer* and end up with a *activation layer*. Although we added more layer after the embedding, we could not make a significant progress.\n",
        "\n",
        "The problem is we dont have a **naive way** of making the model bigger in a productive way. We are still in the case that we crushing all the characters into a single all the way at the begining. And even if we make this a bigger layer and add neurons it's still like silly to squash all that information so fast into a single step.\n",
        "\n",
        "## overview: `WaveNet`\n",
        "\n",
        "![Visualization of the `WaveNet` idea - Progressive Fusion](fig3_wavenet.png)\n",
        "\n",
        "## dataset bump the context size to 8\n",
        "\n",
        "first we change the `block_size` into `8` and now our dataset looks like:\n",
        "\n",
        "``` md\n",
        "........ ---> y\n",
        ".......y ---> u\n",
        "......yu ---> h\n",
        ".....yuh ---> e\n",
        "....yuhe ---> n\n",
        "...yuhen ---> g\n",
        "..yuheng ---> .\n",
        "........ ---> d\n",
        ".......d ---> i\n",
        "......di ---> o\n",
        ".....dio ---> n\n",
        "....dion ---> d\n",
        "...diond ---> r\n",
        "..diondr ---> e\n",
        ".diondre ---> .\n",
        "........ ---> x\n",
        ".......x ---> a\n",
        "......xa ---> v\n",
        ".....xav ---> i\n",
        "....xavi ---> e\n",
        "```\n",
        "\n",
        "The model size now bumps up to 22k.\n",
        "\n",
        "## re-running baseline code on `block_size = 8`\n",
        "\n",
        "Just by lazily extending the context size to 8, we can already improve the model a little bit, the loss on validation split now is around `2.045`. The names generated now look prettier:\n",
        "\n",
        "``` md\n",
        "zamari.\n",
        "brennis.\n",
        "shavia.\n",
        "wililke.\n",
        "obalyid.\n",
        "leenoluja.\n",
        "rianny.\n",
        "jordanoe.\n",
        "yuvalfue.\n",
        "ozleega.\n",
        "jemirene.\n",
        "polton.\n",
        "jawi.\n",
        "meyah.\n",
        "gekiniq.\n",
        "angelinne.\n",
        "tayler.\n",
        "catrician.\n",
        "kyearie.\n",
        "anderias.\n",
        "```\n",
        "\n",
        "Let's deem this as a baseline then we can start to implement `WaveNet` and see how far we can go!\n",
        "\n",
        "## implementing `WaveNet`\n",
        "\n",
        "First, let's revisit the shape of the tensors along the way of the forward pass in our neural net:"
      ],
      "id": "f8e7cd33"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Look at a batch of just 4 examples\n",
        "ix = torch.randint(0, Xtr.shape[0], (4,))\n",
        "Xb, Yb = Xtr[ix], Ytr[ix]\n",
        "logits = model(Xb)\n",
        "print(Xb.shape)\n",
        "Xb\n",
        "\n",
        "# > torch.Size([4, 8]) # because the context length is now 8\n",
        "# > tensor([[ 0,  0,  0,  0,  0,  0, 13,  9],\n",
        "#         [ 0,  0,  0,  0,  0,  0,  0,  0],\n",
        "#         [ 0,  0,  0, 11,  5, 18, 15, 12],\n",
        "#         [ 0,  0,  4, 15, 13,  9, 14,  9]])\n",
        "\n",
        "# Output of the embedding layer, each input is translated\n",
        "# to 10 dimensional vector\n",
        "model.layers[0].out.shape\n",
        "# > torch.Size([4, 8, 10])\n",
        "\n",
        "# Output of Flatten layer, each 10-dim vector is concatenated\n",
        "# to each other for all 8-dim context vectors\n",
        "model.layers[1].out.shape\n",
        "# > torch.Size([4, 80])\n",
        "\n",
        "# Output of the Linear layer, take 80 and create 200 channels,\n",
        "# just via matrix mult\n",
        "model.layers[2].out.shape\n",
        "# > torch.Size([4, 200])"
      ],
      "id": "0055e8be",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now look into the Linear layer, which take the input `x` in the forward pass, multiply by `weight` and add the `bias` in (there is broadcasting here). So the transformation in this layer looks like:"
      ],
      "id": "460e24bf"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "(torch.randn(4, 80) @ torch.randn(80, 200) + torch.randn(200)).shape\n",
        "\n",
        "# > torch.Size([4, 200])"
      ],
      "id": "ab6e4f62",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Input `x` matrix here does not need to be 2 dimensional array. The matrix multiplication in PyTorch is quite powerfull, you can pass more than 2 dimensional array. And all dimensions will be preserved except the last one. Like this:"
      ],
      "id": "13c35aa4"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "(torch.randn(4, 5, 80) @ torch.randn(80, 200) + torch.randn(200)).shape\n",
        "\n",
        "# > torch.Size([4, 5, 200])"
      ],
      "id": "8859add0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Which we want to improve now is not just flatten the 8 characters input too fast at the beginning, we want to group them pair by pair to process them in parallel.\n",
        "\n",
        "``` md\n",
        "(x1 x2) (x3 x4) (x5 x6) (x7 x8)\n",
        "```\n",
        "\n",
        "Particularly for 8 characters block size we want to divide it into 4 groups \\~ 4 pair of *bigrams*. We are increasing **the dimensions of the batch**."
      ],
      "id": "59976a24"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "(torch.randn(4, 4, 20) @ torch.randn(20, 200) + torch.randn(200)).shape\n",
        "\n",
        "# > torch.Size([4, 4, 200])"
      ],
      "id": "12b157a6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "How can we achieve this in PyTorch, we can index the odd and even indexes then pair them up."
      ],
      "id": "d1317130"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "e = torch.randn(4, 8, 10) # 4 examples, 8 chars context size, 10-d embedding\n",
        "# goal: want this to be (4, 4, 20) where consecutive 10-d vectors get concatenated\n",
        "\n",
        "explicit = torch.cat([e[:, ::2, :], e[:, 1::2, :]], dim=2) # cat in the third dim\n",
        "explicit.shape \n",
        "# > torch.Size([4, 4, 20])"
      ],
      "id": "ec1324db",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Of course, PyTorch provide a more efficient way to do this, using `view()`:"
      ],
      "id": "d039cbd0"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "(e.view(4, 4, 20) == explicit).all()\n",
        "# > tensor(True)"
      ],
      "id": "14ee9301",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We are going to modulize the `FlattenConsecutive`:"
      ],
      "id": "51830651"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "class FlattenConsecutive:\n",
        "  \n",
        "  def __init__(self, n):\n",
        "    self.n = n\n",
        "    \n",
        "  def __call__(self, x):\n",
        "    B, T, C = x.shape\n",
        "    x = x.view(B, T//self.n, C*self.n)\n",
        "    if x.shape[1] == 1: # spurious tensor, if it's 1, squeeze it\n",
        "      x = x.squeeze(1)\n",
        "    self.out = x\n",
        "    return self.out\n",
        "  \n",
        "  def parameters(self):\n",
        "    return []"
      ],
      "id": "a28cc2ec",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "and update the flatten layer to `FlattenConsecutive(block_size)` (8) in our `model`. We can observe the dimension of tensors in all layers:"
      ],
      "id": "98e3738d"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "for layer in model.layers:\n",
        "  print(layer.__class__.__name__,\": \", tuple(layer.out.shape))\n",
        "\n",
        "# Embedding :  (4, 8, 10)\n",
        "# Flatten :  (4, 80)\n",
        "# Linear :  (4, 200)\n",
        "# BatchNorm1d :  (4, 200)\n",
        "# Tanh :  (4, 200)\n",
        "# Linear :  (4, 27)"
      ],
      "id": "28ecef73",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This is what we have currently. But as said, we dont want to flatten too fast, so we gonna flatten by 2 character, here is the update of the `model` - 3 layers present the consecutive flatten `4 -> 2 -> 1`:"
      ],
      "id": "54ee75a3"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "model = Sequential([\n",
        "  Embedding(vocab_size, n_embd),\n",
        "  FlattenConsecutive(2), Linear(n_embd * 2, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),\n",
        "  FlattenConsecutive(2), Linear(n_hidden*2, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),\n",
        "  FlattenConsecutive(2), Linear(n_hidden*2, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),\n",
        "  Linear(n_hidden, vocab_size),\n",
        "])"
      ],
      "id": "faa51a1c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "and here is tensors dimension flowing in forward pass:\n",
        "\n",
        "``` md\n",
        "Embedding :  (4, 8, 10)\n",
        "FlattenConsecutive :  (4, 4, 20)\n",
        "Linear :  (4, 4, 200)\n",
        "BatchNorm1d :  (4, 4, 200)\n",
        "Tanh :  (4, 4, 200)\n",
        "FlattenConsecutive :  (4, 2, 400)\n",
        "Linear :  (4, 2, 200)\n",
        "BatchNorm1d :  (4, 2, 200)\n",
        "Tanh :  (4, 2, 200)\n",
        "FlattenConsecutive :  (4, 400)\n",
        "Linear :  (4, 200)\n",
        "BatchNorm1d :  (4, 200)\n",
        "Tanh :  (4, 200)\n",
        "Linear :  (4, 27)\n",
        "```\n",
        "\n",
        "That's is, we have successfully implemented the `WaveNet`.\n",
        "\n",
        "## training the `WaveNet`: first pass\n",
        "\n",
        "Now assume we use the same size of network (number of neurons), let's see if the `loss` can be improved. We change the `n_hidden = 68`, so that the total parameters of our network remain 22k. Below is update tensor dims for a batch (32):\n",
        "\n",
        "``` md\n",
        "Embedding :  (32, 8, 10)\n",
        "FlattenConsecutive :  (32, 4, 20)\n",
        "Linear :  (32, 4, 68)\n",
        "BatchNorm1d :  (32, 4, 68)\n",
        "Tanh :  (32, 4, 68)\n",
        "FlattenConsecutive :  (32, 2, 136)\n",
        "Linear :  (32, 2, 68)\n",
        "BatchNorm1d :  (32, 2, 68)\n",
        "Tanh :  (32, 2, 68)\n",
        "FlattenConsecutive :  (32, 136)\n",
        "Linear :  (32, 68)\n",
        "BatchNorm1d :  (32, 68)\n",
        "Tanh :  (32, 68)\n",
        "Linear :  (32, 27)\n",
        "```\n",
        "\n",
        "It turns out that we got almost identical result. There are 2 things:\n",
        "\n",
        "1.  We just constructed the architecture of `WaveNet` but not tortured the model enough to find best set of hyperparameters; and\n",
        "2.  We may have a bug in `BatchNorm1d` layer, let's take a look into this.\n",
        "\n",
        "## fixing `batchnorm1d` bug\n",
        "\n",
        "Let's look at the `BatchNorm1d` happen in the first flatten layer:"
      ],
      "id": "d1ea8f51"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "e = torch.randn(32, 4 , 68)\n",
        "emean = e.mean(0, keepdim = True) # 1, 4, 68\n",
        "evar = e.var(0, keepdim = True) # 1, 4, 68\n",
        "\n",
        "ehat = (e - emean) / torch.sqrt(evar + 1e-5)\n",
        "ehat.shape\n",
        "\n",
        "# > torch.Size([32, 4, 68])"
      ],
      "id": "057f2b54",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For `ehat`, everything is calculated properly, mean and variance are calculated to the batch and the 2nd dim `4` is preserved. But for the `running_mean`:"
      ],
      "id": "c659c203"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "model.layers[3].running_mean.shape\n",
        "\n",
        "# > torch.Size([1, 4, 68])"
      ],
      "id": "05d3dce6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We see it is (1, 4, 68) while we are expected it's 1 dimensional only which is defined in the init method (`torch.zeros(dim)`). We are maintaining the batch norm in parallel over 4 x 68 channels individually and independently instead of just 68 channels. We want to treat this `4` just like a batch norm dimension, ie everaging of `32 * 4` numbers for 68 channels. Fortunately PyTorch `mean()` method offer the reducing dimension not only for integer but also tuple."
      ],
      "id": "2e7c71b4"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "e = torch.randn(32, 4 , 68)\n",
        "emean = e.mean((0,1), keepdim = True) # 1, 1, 68\n",
        "evar = e.var((0,1), keepdim = True) # 1, 1, 68\n",
        "\n",
        "ehat = (e - emean) / torch.sqrt(evar + 1e-5)\n",
        "ehat.shape\n",
        "\n",
        "# > torch.Size([32, 4, 68])\n",
        "\n",
        "model.layers[3].running_mean.shape\n",
        "\n",
        "# > torch.Size([1, 1, 68])"
      ],
      "id": "8eaa12f0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We now modify the `BatchNorm1d` definition accordingly, only the training mean/var in the `__call__` method:"
      ],
      "id": "98042922"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ---- remains the same\n",
        "  def __call__(self, x):\n",
        "    # calculate the forward pass\n",
        "    if self.training:\n",
        "      if x.ndim == 2:\n",
        "        dim = 0\n",
        "      elif x.ndim == 3:\n",
        "        dim = (0,1)\n",
        "      xmean = x.mean(dim, keepdim=True) # batch mean\n",
        "      xvar = x.var(dim, keepdim=True) # batch variance\n",
        "    else:\n",
        "      xmean = self.running_mean\n",
        "      xvar = self.running_var\n",
        "    xhat = (x - xmean) / torch.sqrt(xvar + self.eps) # normalize to unit variance\n",
        "    self.out = self.gamma * xhat + self.beta\n",
        "# ---- remaind the same"
      ],
      "id": "32f8e2c8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## re-training `WaveNet` with bug fix\n",
        "\n",
        "Now retraining the network with bug fixed, we obtain a slightly better loss of `2.022`. We just fixed the normalization term inside the network so they did not thrush too much so a little improvement only is expected.\n",
        "\n",
        "## scaling up our `WaveNet`\n",
        "\n",
        "Now we're ready to scale up our network and retrain everything, the model now have roughly 76k paramters. We finally passed the `2.0` threshold and achieved the loss of `1.99` on the validation split."
      ],
      "id": "6e435423"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "n_embd = 24\n",
        "n_hidden = 128"
      ],
      "id": "1f5fc7e6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "And here is final loss plot:\n",
        "\n",
        "![`lossi` final](final_lossi.png)\n",
        "\n",
        "# 3 conclusions\n",
        "\n",
        "## performance log\n",
        "\n",
        "+--------------+-----------------------------------------------------------------+--------------------------+\n",
        "| Step         | What we did                                                     | Loss we got (accum)      |\n",
        "+==============+=================================================================+==========================+\n",
        "| 1            | original (3 character context + 200 hidden neurons, 12K params) | train 2.0467958450317383 |\n",
        "|              |                                                                 |                          |\n",
        "|              |                                                                 | val 2.0989298820495605   |\n",
        "+--------------+-----------------------------------------------------------------+--------------------------+\n",
        "| 2            | context: 3 -\\> 8 (22K params)                                   | train 1.9028635025024414 |\n",
        "|              |                                                                 |                          |\n",
        "|              |                                                                 | val 2.044949769973755    |\n",
        "+--------------+-----------------------------------------------------------------+--------------------------+\n",
        "| 3            | flat -\\> hierarchical (22K params)                              | train 1.9366059303283691 |\n",
        "|              |                                                                 |                          |\n",
        "|              |                                                                 | val 2.017268419265747    |\n",
        "+--------------+-----------------------------------------------------------------+--------------------------+\n",
        "| 4            | fix bug in `batchnorm1d`                                        | train 1.9156142473220825 |\n",
        "|              |                                                                 |                          |\n",
        "|              |                                                                 | val 2.0228867530822754   |\n",
        "+--------------+-----------------------------------------------------------------+--------------------------+\n",
        "| 5            | scale up the network: `n_embd` 24, `n_hidden` 128 (76K params)  | train 1.7680459022521973 |\n",
        "|              |                                                                 |                          |\n",
        "|              |                                                                 | val 1.994154691696167    |\n",
        "+--------------+-----------------------------------------------------------------+--------------------------+\n",
        "\n",
        ": Loss logs {tbl-colwidths=\"\\[10,40,50\\]\" .striped .hover}\n",
        "\n",
        "## experimental harness\n",
        "\n",
        "The \"harness\" metaphor is apt because it's like a structured support system that allows researchers to systematically explore and optimize neural network configurations, much like a harness helps guide and support an athlete during training.\n",
        "\n",
        "::: callout-note\n",
        "An experimental harness typically includes several key components:\n",
        "\n",
        "1.  Hyperparameter Search Space Definition: This involves specifying the range of hyperparameters to be explored, such as:\n",
        "\n",
        "-   Learning rates\n",
        "-   Batch sizes\n",
        "-   Network architecture depths\n",
        "-   Activation functions\n",
        "-   Regularization techniques\n",
        "-   Dropout rates\n",
        "\n",
        "2.  Search Strategy: Methods for exploring the hyperparameter space, which can include:\n",
        "\n",
        "-   Grid search\n",
        "-   Random search\n",
        "-   Bayesian optimization\n",
        "-   Evolutionary algorithms\n",
        "-   Gradient-based optimization techniques\n",
        "\n",
        "3.  Evaluation Metrics: Predefined metrics to assess model performance, such as:\n",
        "\n",
        "-   Validation accuracy\n",
        "-   Loss function values\n",
        "-   Precision and recall\n",
        "-   F1 score\n",
        "-   Computational efficiency\n",
        "\n",
        "4.  Automated Experiment Management: Tools and scripts that can:\n",
        "\n",
        "-   Automatically generate and run different model configurations\n",
        "-   Log results\n",
        "-   Track experiments\n",
        "-   Compare performance across different hyperparameter settings\n",
        "\n",
        "5.  Reproducibility Mechanisms: Ensuring that experiments can be repeated and validated, which includes:\n",
        "\n",
        "-   Fixed random seeds\n",
        "-   Consistent data splitting\n",
        "-   Versioning of datasets and configurations\n",
        ":::\n",
        "\n",
        "## `WaveNet` but with “dilated causal convolutions”\n",
        "\n",
        "- Convolution is a \"for loop\" applying a linear filter over  space of some input sequence;\n",
        "- Not happen only in Python but also in Kernel\n",
        "\n",
        "## `torch.nn`\n",
        "\n",
        "We have implement alot of concepts in `torch.nn`:\n",
        "\n",
        "- containers: `Sequential`\n",
        "- `Linear`, `BatchNorm1d`, `Tanh`, `FlattenConsecutive`, ...\n",
        "\n",
        "## the development process of building deep neural nets & going forward\n",
        "\n",
        "+ Spending a ton of time exploring PyTorch documentation, unfortunately it's not a good one;\n",
        "+ Ton of time to make the shapes work: fan in, fan out, NLC or NLC, broadcasting, viewing, etc;\n",
        "+ What we:\n",
        "  - done: implemented dilated causal convoluntional network;\n",
        "  - to be explores: residual and skip connections;\n",
        "  - to be explores: experimental harness;\n",
        "  - more mordern networks: RNN, LSTM, Transformer.\n",
        "\n",
        "# 4 resources\n",
        "\n",
        "1.  WaveNet 2016 from DeepMind: <https://arxiv.org/abs/1609.03499>;\n",
        "2.  Bengio et al. 2003 MLP LM: <https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf>;\n",
        "3.  Notebook: <https://github.com/karpathy/nn-zero-to-hero/blob/master/lectures/makemore/makemore_part5_cnn1.ipynb>;\n",
        "4.  DeepMind's blog post from 2016: <https://deepmind.google/discover/blog/wavenet-a-generative-model-for-raw-audio/>"
      ],
      "id": "257eff2b"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}