{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"NN-Z2H Lesson 5: Building makemore part 4 - Becoming a Backprop Ninja\"\n",
        "description: \"Become swole doge: build a 2-layer MLP and deep dive to how gradients flow backwards with `cross entropy loss`, 2nd linear layer, `tanh`, `batchnorm`, 1st linear layer, and the `embedding table`.\"\n",
        "author:\n",
        "  - name: \"Tuan Le Khac\"\n",
        "    url: https://lktuan.github.io/\n",
        "categories: [til, python, andrej karpathy, nn-z2h, neural networks, backpropagation] \n",
        "date: 12-02-2024\n",
        "date-modified: 12-03-2024\n",
        "image: backprop_ninja.jpg\n",
        "draft: false\n",
        "format:\n",
        "  html:\n",
        "    code-overflow: wrap\n",
        "    code-tools: true\n",
        "    code-fold: show\n",
        "    code-annotations: hover\n",
        "    mermaid:\n",
        "      theme: neutral\n",
        "---"
      ],
      "id": "5996c7a3"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "::: {.callout-important title=\"This is not orginal content!\"}\n",
        "This is my study notes / codes along with Andrej Karpathy's \"[Neural Networks: Zero to Hero](https://www.youtube.com/watch?v=VMj-3S1tku0&list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ)\" series.\n",
        ":::\n",
        "\n",
        "# intro: why you should care\n",
        "\n",
        "![swole doge style backpropagation, image credit to [this video](https://www.youtube.com/watch?v=LzxnmqnctmA)](swole_doggo.jpg)\n",
        "\n",
        "In previous lecture, we're introduced to some common issues with our \"shallow\" (and of course for deep as well) neural network and how to fix with the initialization setting and Batch Normalization. We're also learnt some diagnostic tools to observe *forward pass activations*, *backward pass gradients*, and *weights update*, to calibrate the training loop. In this lacture, we aim to replace this line of code:"
      ],
      "id": "ec84136b"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| eval: false\n",
        "... loss.backward() ..."
      ],
      "id": "39e389a2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "with from-scratch-code. It's basically identical to `MicroGrad`, but on *Tensor* rather than *Scalar*. Why?\n",
        "\n",
        "> The problem with Backpropagation is that it is a [leaky abstraction](https://en.wikipedia.org/wiki/Leaky_abstraction). -- Andrej Karpathy\n",
        "\n",
        "readmore: <https://karpathy.medium.com/yes-you-should-understand-backprop-e2f06eab496b>, and <https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html>\n",
        "\n",
        "inshort, to effectively debug neural network, we should be deeply understanding how back propagation work under the hood. Let's do it!\n",
        "\n",
        "# starter code\n",
        "\n",
        "### import libraries:"
      ],
      "id": "d0184876"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "id": "94a8c398",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### read data:"
      ],
      "id": "b3eb74fd"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import pandas as pd\n",
        "\n",
        "url = \"https://raw.githubusercontent.com/karpathy/makemore/refs/heads/master/names.txt\"\n",
        "words = pd.read_csv(url, header=None).iloc[:, 0].tolist()\n",
        "words[:8]"
      ],
      "id": "8d0615c3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### build vocab:"
      ],
      "id": "1c83017e"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# build the vocabulary of characters and mapping to/from integer\n",
        "chars = sorted(list(set(''.join(words))))\n",
        "stoi = {s:i+1 for i, s in enumerate(chars)}\n",
        "stoi['.'] = 0\n",
        "itos = {i: s for s, i in stoi.items()}\n",
        "vocab_size = len(itos)\n",
        "print(itos)\n",
        "print(vocab_size)"
      ],
      "id": "7f7eb7d6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### build dataset splits (identical to previous so I folded it):"
      ],
      "id": "f152a0e7"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| code-fold: true\n",
        "block_size = 3 # context length: how many characters do we take to predict the next one.\n",
        "# build the dataset\n",
        "def buid_dataset(words):\n",
        "    X, Y = [], []\n",
        "\n",
        "    for w in words:\n",
        "        context = [0] * block_size\n",
        "        for ch in w + '.':\n",
        "            ix = stoi[ch]\n",
        "            X.append(context)\n",
        "            Y.append(ix)\n",
        "            context = context[1:] + [ix]\n",
        "\n",
        "    X = torch.tensor(X)\n",
        "    Y = torch.tensor(Y)\n",
        "    print(X.shape, Y.shape)\n",
        "    return X, Y\n",
        "\n",
        "import random\n",
        "random.seed(42)\n",
        "random.shuffle(words)\n",
        "n1 = int(0.8 * len(words))\n",
        "n2 = int(0.9 * len(words))\n",
        "\n",
        "Xtr, Ytr = buid_dataset(words[:n1])        # 80#\n",
        "Xdev, Ydev = buid_dataset(words[n1:n2])    # 10%\n",
        "Xte, Yte = buid_dataset(words[n2:])        # 10%"
      ],
      "id": "b5ae6350",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### utility function we will use later when comparing manual gradients to PyTorch gradients."
      ],
      "id": "2fcc890a"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def cmp(s, dt, t):\n",
        "  ex = torch.all(dt == t.grad).item()\n",
        "  app = torch.allclose(dt, t.grad)\n",
        "  maxdiff = (dt - t.grad).abs().max().item()\n",
        "  print(f'{s:15s} | exact: {str(ex):5s} | approximate: {str(app):5s} | maxdiff: {maxdiff}')"
      ],
      "id": "f214ec71",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### network construction:"
      ],
      "id": "d3b4bcc3"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "n_embd = 10 # the dimensionality of the character embedding vectors\n",
        "n_hidden = 64 # the number of neurons in the hidden layer of the MLP\n",
        "\n",
        "g = torch.Generator().manual_seed(2147483647) # for reproducibility\n",
        "C  = torch.randn((vocab_size, n_embd),            generator=g)\n",
        "# Layer 1\n",
        "W1 = torch.randn((n_embd * block_size, n_hidden), generator=g) * (5/3)/((n_embd * block_size)**0.5)\n",
        "b1 = torch.randn(n_hidden,                        generator=g) * 0.1 # using b1 just for fun, it's useless because of BN\n",
        "# Layer 2\n",
        "W2 = torch.randn((n_hidden, vocab_size),          generator=g) * 0.1\n",
        "b2 = torch.randn(vocab_size,                      generator=g) * 0.1\n",
        "# BatchNorm parameters\n",
        "bngain = torch.randn((1, n_hidden))*0.1 + 1.0\n",
        "bnbias = torch.randn((1, n_hidden))*0.1\n",
        "\n",
        "# Note: I am initializating many of these parameters in non-standard ways\n",
        "# because sometimes initializating with e.g. all zeros could mask an incorrect\n",
        "# implementation of the backward pass.\n",
        "\n",
        "parameters = [C, W1, b1, W2, b2, bngain, bnbias]\n",
        "print(sum(p.nelement() for p in parameters)) # number of parameters in total\n",
        "for p in parameters:\n",
        "  p.requires_grad = True"
      ],
      "id": "be70e3e5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### mini-batch construction:"
      ],
      "id": "4a29344d"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "batch_size = 32\n",
        "n = batch_size # a shorter variable also, for convenience\n",
        "# construct a minibatch\n",
        "ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n",
        "Xb, Yb = Xtr[ix], Ytr[ix] # batch X,Y"
      ],
      "id": "91394aec",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### forward pass, \"chunkated\" into smaller steps that are possible to backward one at a time."
      ],
      "id": "13bf1fcf"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "emb = C[Xb] # embed the characters into vectors\n",
        "embcat = emb.view(emb.shape[0], -1) # concatenate the vectors\n",
        "\n",
        "# Linear layer 1\n",
        "hprebn = embcat @ W1 + b1 # hidden layer pre-activation\n",
        "\n",
        "# BatchNorm layer\n",
        "bnmeani = 1/n*hprebn.sum(0, keepdim=True)\n",
        "bndiff = hprebn - bnmeani\n",
        "bndiff2 = bndiff**2\n",
        "bnvar = 1/(n-1)*(bndiff2).sum(0, keepdim=True) # note: Bessel's correction (dividing by n-1, not n)\n",
        "bnvar_inv = (bnvar + 1e-5)**-0.5\n",
        "bnraw = bndiff * bnvar_inv\n",
        "hpreact = bngain * bnraw + bnbias\n",
        "\n",
        "# Non-linearity\n",
        "h = torch.tanh(hpreact) # hidden layer\n",
        "\n",
        "# Linear layer 2\n",
        "logits = h @ W2 + b2 # output layer\n",
        "\n",
        "# cross entropy loss (same as F.cross_entropy(logits, Yb)), Kullback–Leibler divergence\n",
        "logit_maxes = logits.max(1, keepdim=True).values\n",
        "norm_logits = logits - logit_maxes # subtract max for numerical stability\n",
        "counts = norm_logits.exp()\n",
        "counts_sum = counts.sum(1, keepdims=True)\n",
        "counts_sum_inv = counts_sum**-1 # if I use (1.0 / counts_sum) instead then I can't get backprop to be bit exact...\n",
        "probs = counts * counts_sum_inv\n",
        "logprobs = probs.log()\n",
        "loss = -logprobs[range(n), Yb].mean()\n",
        "\n",
        "# PyTorch backward pass\n",
        "for p in parameters:\n",
        "  p.grad = None\n",
        "for t in [logprobs, probs, counts, counts_sum, counts_sum_inv, # afaik there is no cleaner way\n",
        "          norm_logits, logit_maxes, logits, h, hpreact, bnraw,\n",
        "         bnvar_inv, bnvar, bndiff2, bndiff, hprebn, bnmeani,\n",
        "         embcat, emb]:\n",
        "  t.retain_grad()\n",
        "\n",
        "hprebn.retain_grad() # Tuan added this line since code above does not ensure hprebn's grad to be retained.\n",
        "\n",
        "loss.backward()\n",
        "loss"
      ],
      "id": "85f6dab7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# exercise 1: backproping the atomic compute graph\n",
        "\n",
        "I can do 60, 70% of this myself \\~ for popular mathematics operations. What I need to remember after this excercise are backwards of:\n",
        "\n",
        "-   elements in matrix mult;\n",
        "-   max operator;\n",
        "-   indexing operator; and\n",
        "-   broadcasting behavior.\n",
        "\n",
        "![Andrej inductive reasoning, explains how to get a derivative from matrix multiplication.](explain_matrix_mult_grad.png)\n",
        "\n",
        "Given the matrix mult expression $d = a @ b + c$, we have:\n",
        "\n",
        "-   $\\frac{\\partial L}{\\partial \\mathbf{a}} = \\frac{\\partial L}{\\partial \\mathbf{d}} @ \\mathbf{b}^T$\n",
        "-   $\\frac{\\partial L}{\\partial \\mathbf{b}} = \\mathbf{a}^T @ \\frac{\\partial L}{\\partial \\mathbf{d}}$\n",
        "-   $\\frac{\\partial L}{\\partial \\mathbf{c}} = \\frac{\\partial L}{\\partial \\mathbf{d}}.sum(0)$"
      ],
      "id": "50fff56f"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Exercise 1: backprop through the whole thing manually, \n",
        "# backpropagating through exactly all of the variables \n",
        "# as they are defined in the forward pass above, one by one\n",
        "print(\"Cross entropy loss\".upper())\n",
        "dlogprobs = torch.zeros_like(logprobs) # create zeros tensor with same size\n",
        "dlogprobs[range(n), Yb] = - 1.0 / n # <1>\n",
        "dprobs = torch.zeros_like(probs) \n",
        "dprobs = (1.0 / probs) * dlogprobs # <2>\n",
        "dcounts_sum_inv = (counts * dprobs).sum(1, keepdim=True) # <3>\n",
        "dcounts_sum = (-1 * counts_sum**-2) * dcounts_sum_inv\n",
        "dcounts = torch.ones_like(counts) * dcounts_sum + counts_sum_inv * dprobs # <4>\n",
        "dnorm_logits = (norm_logits.exp()) * dcounts # = counts * dcounts\n",
        "dlogit_maxes = (- dnorm_logits).sum(1, keepdim=True) # broadcasting, again\n",
        "\n",
        "cmp('logprobs', dlogprobs, logprobs)\n",
        "cmp('probs', dprobs, probs)\n",
        "cmp('counts_sum_inv', dcounts_sum_inv, counts_sum_inv)\n",
        "cmp('counts_sum', dcounts_sum, counts_sum)\n",
        "cmp('counts', dcounts, counts)\n",
        "cmp('norm_logits', dnorm_logits, norm_logits)\n",
        "cmp('logit_maxes', dlogit_maxes, logit_maxes)\n",
        "\n",
        "\n",
        "dlogits = (dnorm_logits.clone() \n",
        "          + F.one_hot( logits.max(1).indices, # broadcasting, again\n",
        "                        num_classes=logits.shape[1]\n",
        "                        ) * dlogit_maxes) # <5>\n",
        "\n",
        "print(\"Layer 2 with Linear and Non-linearity tanh\".upper())\n",
        "dh =  dlogits @ W2.T\n",
        "dW2 = h.T @ dlogits\n",
        "db2 = dlogits.sum(0) # sum by column, vertical, to eliminate the 0-index dim\n",
        "\n",
        "cmp('logits', dlogits, logits)\n",
        "cmp('h', dh, h)\n",
        "cmp('W2', dW2, W2)\n",
        "cmp('b2', db2, b2)\n",
        "\n",
        "print(\"Batch Norm layer\".upper())\n",
        "dhpreact = (1 - h**2) * dh # output of the tanh, square\n",
        "dbngain = (bnraw * dhpreact).sum(0, keepdim=True) # <6>\n",
        "dbnbias = dhpreact.sum(0, keepdim=True)\n",
        "dbnraw = bngain * dhpreact\n",
        "dbnvar_inv = (bndiff * dbnraw).sum(0, keepdim=True) # <7>\n",
        "dbnvar = (-0.5 * (bnvar + 1e-5)**-1.5) * dbnvar_inv\n",
        "dbndiff2 = 1/(n-1) * torch.ones_like(bndiff2) * dbnvar\n",
        "dbndiff = bnvar_inv * dbnraw + 2 * bndiff * dbndiff2\n",
        "dbnmeani = (- torch.ones_like(dbndiff) * dbndiff).sum(0)\n",
        "\n",
        "cmp('hpreact', dhpreact, hpreact)\n",
        "cmp('bngain', dbngain, bngain)\n",
        "cmp('bnbias', dbnbias, bnbias)\n",
        "cmp('bnraw', dbnraw, bnraw)\n",
        "cmp('bnvar_inv', dbnvar_inv, bnvar_inv)\n",
        "cmp('bnvar', dbnvar, bnvar)\n",
        "cmp('bndiff2', dbndiff2, bndiff2)\n",
        "cmp('bndiff', dbndiff, bndiff)\n",
        "cmp('bnmeani', dbnmeani, bnmeani)\n",
        "\n",
        "print(\"Linear layer 1\".upper())\n",
        "dhprebn = dbndiff.clone() + (1.0/n * (torch.ones_like(hprebn) * dbnmeani))\n",
        "dembcat = dhprebn @ W1.T\n",
        "dW1 = embcat.T @ dhprebn\n",
        "db1 = dhprebn.sum(0, keepdim=True)\n",
        "demb = dembcat.view(emb.shape)                         # <8>\n",
        "dC = torch.zeros_like(C)                               # <9>\n",
        "for k in range(Xb.shape[0]): # iterate all elements of the Xb\n",
        "  for j in range(Xb.shape[1]):\n",
        "    ix = Xb[k,j]\n",
        "    dC[ix] += demb[k,j]\n",
        "# Method 2\n",
        "dC = torch.zeros_like(C)\n",
        "dC.index_add_(0, Xb.view(-1), demb.view(-1, 10))\n",
        "\n",
        "cmp('hprebn', dhprebn, hprebn)\n",
        "cmp('embcat', dembcat, embcat)\n",
        "cmp('W1', dW1, W1)\n",
        "cmp('b1', db1, b1)\n",
        "cmp('emb', demb, emb)\n",
        "cmp('C', dC, C)"
      ],
      "id": "ba143190",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "1.  `dlogprobs`: `logprobs` is an (32, 27) array - contains log probabilities of every character in 27 vocab for 32 output. `loss` just indexes (`(range(n), Yb)`) to pick out the corresponding numbers of the ground trues, then do mean (`1 / n`) and get negative (`-`). So the grad for each element that had been picked out is `-1/n`, while for others is `0`;\n",
        "2.  `dprobs`: d/dx log(x) is just `1/x` - local derivative, then multiply by next leaf grad `dlogprobs`, element wise;\n",
        "3.  `dcounts_sum_inv`: should be `counts * dprobs` according to chainrule. But remember `counts.shape` is (32, 27), and `counts_sum_inv.shape` is (32, 1), then there is **broadcasting** to 27 columns. `counts_sum_inv` is being used multiple times in the topo/diagram -\\> we need to sum them (grad) up. We do it by columns so `Keepdim=True`;\n",
        "4.  `dcount`: was used in 2 expression, so it would be the sum of (1) `counts_sum_inv * dprobs` - same with `dcounts_sum_inv` by symmetry, and (2) `torch.ones_like(counts) * dcounts_sum` - the gradient flow from `dcounts_sum` equally, and equal to `1`;\n",
        "5.  `dlogits`: sum of 2 flows, the 2nd one for `max()` operations -\\> gradient should be the `1` for those max elements, the remain would be `0`;\n",
        "6.  Notice the **vertical broadcasting** of `bngain` and `bnbias`;\n",
        "7.  Need to sum up vertically because `bnvar_inv` is (1, 64) while 2 multipliers are (32, 64);\n",
        "8.  Undo the concatenation;\n",
        "9.  Undo the indexing: `emb.shape = (32, 3, 10)`, `C.shape = (27, 10)`, `Xb.shape = (32, 3)`.\n",
        "\n",
        "# brief digression: bessel’s correction in batchnorm\n",
        "\n",
        "The paper of Batch Norm inconsistantly mentioned that:\n",
        "\n",
        "-   they used biased variance for training;\n",
        "-   and used un-biased variance for inference.\n",
        "\n",
        "We train on small mini-batch, so should be using un-biased variance. PyTorch, since implemented what exactly paper wrote, has this discrepancy.\n",
        "\n",
        "# exercise 2: cross entropy loss backward pass\n",
        "\n",
        "We are realizing that we doing too much work since we need to break the `loss` calculation from `logits` into too many steps that (1) they are easy enough for us to do backpropagation, but (2) most of them can cancel each other out. So in exercise two, we need to convert those bunch of atomic pieces of calculation to a shorter formula of cross entropy that can facilitate the backpropating."
      ],
      "id": "cf22e3f5"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Exercise 2: backprop through cross_entropy but all in one go\n",
        "# to complete this challenge look at the mathematical expression of the loss,\n",
        "# take the derivative, simplify the expression, and just write it out\n",
        "\n",
        "# forward pass\n",
        "\n",
        "# before:\n",
        "# logit_maxes = logits.max(1, keepdim=True).values\n",
        "# norm_logits = logits - logit_maxes # subtract max for numerical stability\n",
        "# counts = norm_logits.exp()\n",
        "# counts_sum = counts.sum(1, keepdims=True)\n",
        "# counts_sum_inv = counts_sum**-1 # if I use (1.0 / counts_sum) instead then I can't get backprop to be bit exact...\n",
        "# probs = counts * counts_sum_inv\n",
        "# logprobs = probs.log()\n",
        "# loss = -logprobs[range(n), Yb].mean()\n",
        "\n",
        "# now:\n",
        "loss_fast = F.cross_entropy(logits, Yb)\n",
        "print(loss_fast.item(), 'diff:', (loss_fast - loss).item())"
      ],
      "id": "7d3511f5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Mathematically given the Cross-Entropy loss formula:\n",
        "\n",
        "$L = -\\frac{1}{n} \\sum_{i=1}^{n} \\sum_{j=1}^{k} Y_{ij} \\log(\\hat{Y}_{ij})$\n",
        "\n",
        "with $\\hat{Y}{ij}$ is calculated by the softmax transformation:\n",
        "\n",
        "$\\hat{Y}{ij} = \\frac{\\exp(\\text{logits}{ij})}{\\sum_{c=1}^{k} \\exp(\\text{logits}_{ic})}$\n",
        "\n",
        "where:\n",
        "\n",
        "-   $n$: batch size, in this case is 32 - `n`;\n",
        "-   $k$: vocab size or number of classes, in this case is 27 - `vocab_size`;\n",
        "-   $Y \\in {0,1}^{n \\times k}$: one-hot encoding maxtrix (ground truth) - `Y`;\n",
        "-   $\\text{logits} \\in \\mathbb{R}^{n \\times k}$: raw logits, input of softmax layer - `logits`;\n",
        "-   $\\hat{Y} \\in [0,1]^{n \\times k}$: probabilities after softmax layer - `probs`.\n",
        "\n",
        "I actually can not do this exercise so AK's solution here:"
      ],
      "id": "0c2c36d7"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# backward pass\n",
        "\n",
        "dlogits = F.softmax(logits, 1)\n",
        "dlogits[range(n), Yb] -= 1.0\n",
        "dlogits *= n**-1.0\n",
        "\n",
        "cmp('logits', dlogits, logits) # I can only get approximate to be true, my maxdiff is 6e-9"
      ],
      "id": "92a5f538",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "I will comeback with another post on thisss: the backward for cross entropy loss.\n",
        "\n",
        "### what is `dlogits` intuitively?\n",
        "\n",
        "Now let's look how `dlogits` look like.\n",
        "\n",
        "This is the first row of `logits` through softmax layer, it's probabilities of every possible character in vocab size 27, they are all small and sum of them is 1."
      ],
      "id": "e7170a75"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "F.softmax(logits, 1)[0]"
      ],
      "id": "ed9244a9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "And this is the first row of `dlogits` multiplied by `n` for comparision, it's all identical excep the 8th probability (`Xb[0] = 8`). **And sum of them is 0!**"
      ],
      "id": "63e970d9"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "dlogits[0] * n"
      ],
      "id": "1a626e8b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "So for each data point in the batch of 32, we pushnish super hard the correct character, making the magnitude of it's grad is so high (negative number), then the prob of that predict-character can change toward to correct one. This push and pull is somehow the way that network learn."
      ],
      "id": "07ae6f84"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "plt.figure(figsize=(8, 8))\n",
        "plt.imshow(dlogits.detach(), cmap=\"gray\")"
      ],
      "id": "6912acb5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "When the network perfectly predict, softmax will have a full row vector except the 8th prob, which is 1. Then the `dlogits` will be full of 0, the network stop learning.\n",
        "\n",
        "# exercise 3: batch norm layer backward pass"
      ],
      "id": "002e4299"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Exercise 3: backprop through batchnorm but all in one go\n",
        "# to complete this challenge look at the mathematical expression of the output of batchnorm,\n",
        "# take the derivative w.r.t. its input, simplify the expression, and just write it out\n",
        "\n",
        "# forward pass\n",
        "\n",
        "# before:\n",
        "# bnmeani = 1/n*hprebn.sum(0, keepdim=True)\n",
        "# bndiff = hprebn - bnmeani\n",
        "# bndiff2 = bndiff**2\n",
        "# bnvar = 1/(n-1)*(bndiff2).sum(0, keepdim=True) # note: Bessel's correction (dividing by n-1, not n)\n",
        "# bnvar_inv = (bnvar + 1e-5)**-0.5\n",
        "# bnraw = bndiff * bnvar_inv\n",
        "# hpreact = bngain * bnraw + bnbias\n",
        "\n",
        "# now:\n",
        "hpreact_fast = bngain * (hprebn - hprebn.mean(0, keepdim=True)) / torch.sqrt(hprebn.var(0, keepdim=True, unbiased=True) + 1e-5) + bnbias\n",
        "print('max diff:', (hpreact_fast - hpreact).abs().max())"
      ],
      "id": "5bd642c2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Below is the image of Batch Norm algorithm:\n",
        "\n",
        "![Batch Normalization algo, [source](https://arxiv.org/abs/1502.03167)](bn_algo.png)\n",
        "\n",
        "And this is flowchart\n",
        "```{mermaid}\n",
        "flowchart LR\n",
        "\n",
        "Input(...) --> X(x)\n",
        "X(x) -- \"(4) dL/dx\" --> Mu(mu)\n",
        "X(x) -- \"(4) dL/dx\" --> Si(\"sigma (square)\")\n",
        "X(x) -- \"(4) dL/dx\" --> Xh(x_hat)\n",
        "Si(\"sigma (square)\") -- \"(2) dL/dsigma\" --> Xh(x_hat)\n",
        "Mu(mu) -- \"(3.1) dL/dmu\" --> Xh(x_hat)\n",
        "Mu(mu) -- \"(3.2) dL/dmu\" --> Si(\"sigma (square)\")\n",
        "Xh(x_hat) -- \"(1) dL/dx_hat\" --> Y(y)\n",
        "\n",
        "G(gamma) --> Y(y)\n",
        "B(beta) --> Y(y)\n",
        "```\n",
        "\n",
        "We will need to calculate `dL/dx` given `dL/dy`, we will calculate by hand reversely:\n",
        "\n",
        "1.  scale and shift: `dL/dx_hat = gamma * dL/dy` easy enough;\n",
        "2.  normalize: `dL/dsigma = -1/2 * gamma * SUM[dL/dy * (x - mu) * (sigma^2 + eps)^(-3/2)]`;\n",
        "3.  normalize & mini-batch variance: `dL/dmu = - SUM[dL/dy * gamma * (sigma^2 + eps)^(-1/2)]`, this is path (3.1), we can prove that path (3.2) equal to zero - since `mu` is average of `x`, we can think of change in `mu` will be eliminated by `x` itself w.r.t. `L`;\n",
        "4.  given all the gradients above, we just write the expressions down and do some transformation, this is the final: `dL/dx = gamma * (sigma^2 + eps)^(-1/2) / m { [m * dL/dy] - SUMj[dL/dy] - m/(m-1) * x_hat * SUMj[dL/dy * x_hat]}`, where `m` is mini-batch size (in our data is `n`)\n"
      ],
      "id": "c27f8b84"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# backward pass\n",
        "\n",
        "# before we had:\n",
        "# dbnraw = bngain * dhpreact\n",
        "# dbndiff = bnvar_inv * dbnraw\n",
        "# dbnvar_inv = (bndiff * dbnraw).sum(0, keepdim=True)\n",
        "# dbnvar = (-0.5*(bnvar + 1e-5)**-1.5) * dbnvar_inv\n",
        "# dbndiff2 = (1.0/(n-1))*torch.ones_like(bndiff2) * dbnvar\n",
        "# dbndiff += (2*bndiff) * dbndiff2\n",
        "# dhprebn = dbndiff.clone()\n",
        "# dbnmeani = (-dbndiff).sum(0)\n",
        "# dhprebn += 1.0/n * (torch.ones_like(hprebn) * dbnmeani)\n",
        "\n",
        "# calculate dhprebn given dhpreact (i.e. backprop through the batchnorm)\n",
        "# (you'll also need to use some of the variables from the forward pass up above)\n",
        "\n",
        "dhprebn = bngain * bnvar_inv / n * ( n * dhpreact - dhpreact.sum(0)  - n / (n-1) * bnraw * (dhpreact * bnraw).sum(0))\n",
        "\n",
        "cmp('hprebn', dhprebn, hprebn) # I can only get approximate to be true, my maxdiff is 9e-10"
      ],
      "id": "b37ba627",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# exercise 4: putting it all together"
      ],
      "id": "5a26a182"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| eval: false\n",
        "# Exercise 4: putting it all together!\n",
        "# Train the MLP neural net with your own backward pass\n",
        "\n",
        "# init\n",
        "n_embd = 10 # the dimensionality of the character embedding vectors\n",
        "n_hidden = 200 # the number of neurons in the hidden layer of the MLP\n",
        "\n",
        "g = torch.Generator().manual_seed(2147483647) # for reproducibility\n",
        "C  = torch.randn((vocab_size, n_embd),            generator=g)\n",
        "# Layer 1\n",
        "W1 = torch.randn((n_embd * block_size, n_hidden), generator=g) * (5/3)/((n_embd * block_size)**0.5)\n",
        "b1 = torch.randn(n_hidden,                        generator=g) * 0.1\n",
        "# Layer 2\n",
        "W2 = torch.randn((n_hidden, vocab_size),          generator=g) * 0.1\n",
        "b2 = torch.randn(vocab_size,                      generator=g) * 0.1\n",
        "# BatchNorm parameters\n",
        "bngain = torch.randn((1, n_hidden))*0.1 + 1.0\n",
        "bnbias = torch.randn((1, n_hidden))*0.1\n",
        "\n",
        "parameters = [C, W1, b1, W2, b2, bngain, bnbias]\n",
        "print(sum(p.nelement() for p in parameters)) # number of parameters in total\n",
        "for p in parameters:\n",
        "  p.requires_grad = True\n",
        "\n",
        "# same optimization as last time\n",
        "max_steps = 200000\n",
        "batch_size = 32\n",
        "n = batch_size # convenience\n",
        "lossi = []\n",
        "\n",
        "# use this context manager for efficiency once your backward pass is written (TODO)\n",
        "with torch.no_grad():\n",
        "\n",
        "  # kick off optimization\n",
        "  for i in range(max_steps):\n",
        "\n",
        "    # minibatch construct\n",
        "    ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n",
        "    Xb, Yb = Xtr[ix], Ytr[ix] # batch X,Y\n",
        "\n",
        "    # forward pass\n",
        "    emb = C[Xb] # embed the characters into vectors\n",
        "    embcat = emb.view(emb.shape[0], -1) # concatenate the vectors\n",
        "    # Linear layer\n",
        "    hprebn = embcat @ W1 + b1 # hidden layer pre-activation\n",
        "    # BatchNorm layer\n",
        "    # -------------------------------------------------------------\n",
        "    bnmean = hprebn.mean(0, keepdim=True)\n",
        "    bnvar = hprebn.var(0, keepdim=True, unbiased=True)\n",
        "    bnvar_inv = (bnvar + 1e-5)**-0.5\n",
        "    bnraw = (hprebn - bnmean) * bnvar_inv\n",
        "    hpreact = bngain * bnraw + bnbias\n",
        "    # -------------------------------------------------------------\n",
        "    # Non-linearity\n",
        "    h = torch.tanh(hpreact) # hidden layer\n",
        "    logits = h @ W2 + b2 # output layer\n",
        "    loss = F.cross_entropy(logits, Yb) # loss function\n",
        "\n",
        "    # backward pass\n",
        "    for p in parameters:\n",
        "      p.grad = None\n",
        "    # loss.backward() # use this for correctness comparisons, delete it later!\n",
        "\n",
        "    # manual backprop! #swole_doge_meme\n",
        "    # -----------------\n",
        "    dlogits = F.softmax(logits, 1)\n",
        "    dlogits[range(n), Yb] -= 1\n",
        "    dlogits /= n\n",
        "    # 2nd layer backprop\n",
        "    dh = dlogits @ W2.T\n",
        "    dW2 = h.T @ dlogits\n",
        "    db2 = dlogits.sum(0)\n",
        "    # tanh\n",
        "    dhpreact = (1.0 - h**2) * dh\n",
        "    # batchnorm backprop\n",
        "    dbngain = (bnraw * dhpreact).sum(0, keepdim=True)\n",
        "    dbnbias = dhpreact.sum(0, keepdim=True)\n",
        "    dhprebn = bngain*bnvar_inv/n * (n*dhpreact - dhpreact.sum(0) - n/(n-1)*bnraw*(dhpreact*bnraw).sum(0))\n",
        "    # 1st layer\n",
        "    dembcat = dhprebn @ W1.T\n",
        "    dW1 = embcat.T @ dhprebn\n",
        "    db1 = dhprebn.sum(0)\n",
        "    # embedding\n",
        "    demb = dembcat.view(emb.shape)\n",
        "    dC = torch.zeros_like(C)\n",
        "    for k in range(Xb.shape[0]):\n",
        "      for j in range(Xb.shape[1]):\n",
        "        ix = Xb[k,j]\n",
        "        dC[ix] += demb[k,j]\n",
        "    grads = [dC, dW1, db1, dW2, db2, dbngain, dbnbias]\n",
        "    # -----------------\n",
        "\n",
        "    # update\n",
        "    lr = 0.1 if i < 100000 else 0.01 # step learning rate decay\n",
        "    for p, grad in zip(parameters, grads):\n",
        "      #p.data += -lr * p.grad # old way of cheems doge (using PyTorch grad from .backward())\n",
        "      p.data += -lr * grad # new way of swole doge TODO: enable\n",
        "\n",
        "    # track stats\n",
        "    if i % 10000 == 0: # print every once in a while\n",
        "      print(f'{i:7d}/{max_steps:7d}: {loss.item():.4f}')\n",
        "    lossi.append(loss.log10().item())\n",
        "\n",
        "    # if i >= 100: # TODO: delete early breaking when you're ready to train the full net\n",
        "    #   break"
      ],
      "id": "81369a3e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can check gradients using this:"
      ],
      "id": "1e67953e"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| eval: false\n",
        "# useful for checking your gradients\n",
        "for p,g in zip(parameters, grads):\n",
        "  # g.requires_grad = True\n",
        "  # g.retain_grad()\n",
        "  cmp(str(tuple(p.shape)), g, p)"
      ],
      "id": "0d430348",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Calibrate the batch norm at the end of training:"
      ],
      "id": "b61a873a"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "with torch.no_grad():\n",
        "  # pass the training set through\n",
        "  emb = C[Xtr]\n",
        "  embcat = emb.view(emb.shape[0], -1)\n",
        "  hpreact = embcat @ W1 + b1\n",
        "  # measure the mean/std over the entire training set\n",
        "  bnmean = hpreact.mean(0, keepdim=True)\n",
        "  bnvar = hpreact.var(0, keepdim=True, unbiased=True)"
      ],
      "id": "1479ff9b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Evaluate train and val loss:"
      ],
      "id": "502bfb69"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "@torch.no_grad() # this decorator disables gradient tracking\n",
        "def split_loss(split):\n",
        "  x,y = {\n",
        "    'train': (Xtr, Ytr),\n",
        "    'val': (Xdev, Ydev),\n",
        "    'test': (Xte, Yte),\n",
        "  }[split]\n",
        "  emb = C[x] # (N, block_size, n_embd)\n",
        "  embcat = emb.view(emb.shape[0], -1) # concat into (N, block_size * n_embd)\n",
        "  hpreact = embcat @ W1 + b1\n",
        "  hpreact = bngain * (hpreact - bnmean) * (bnvar + 1e-5)**-0.5 + bnbias\n",
        "  h = torch.tanh(hpreact) # (N, n_hidden)\n",
        "  logits = h @ W2 + b2 # (N, vocab_size)\n",
        "  loss = F.cross_entropy(logits, y)\n",
        "  print(split, loss.item())\n",
        "\n",
        "split_loss('train')\n",
        "split_loss('val')"
      ],
      "id": "f4057e48",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Similar to what we achieved before!\n",
        "\n",
        "Sample from model:"
      ],
      "id": "c0017328"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# sample from the model\n",
        "g = torch.Generator().manual_seed(2147483647 + 10)\n",
        "\n",
        "for _ in range(20):\n",
        "    \n",
        "    out = []\n",
        "    context = [0] * block_size # initialize with all ...\n",
        "    while True:\n",
        "      # ------------\n",
        "      # forward pass:\n",
        "      # Embedding\n",
        "      emb = C[torch.tensor([context])] # (1,block_size,d)      \n",
        "      embcat = emb.view(emb.shape[0], -1) # concat into (N, block_size * n_embd)\n",
        "      hpreact = embcat @ W1 + b1\n",
        "      hpreact = bngain * (hpreact - bnmean) * (bnvar + 1e-5)**-0.5 + bnbias\n",
        "      h = torch.tanh(hpreact) # (N, n_hidden)\n",
        "      logits = h @ W2 + b2 # (N, vocab_size)\n",
        "      # ------------\n",
        "      # Sample\n",
        "      probs = F.softmax(logits, dim=1)\n",
        "      ix = torch.multinomial(probs, num_samples=1, generator=g).item()\n",
        "      context = context[1:] + [ix]\n",
        "      out.append(ix)\n",
        "      if ix == 0:\n",
        "        break\n",
        "    \n",
        "    print(''.join(itos[i] for i in out))"
      ],
      "id": "80a320f9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# outro\n",
        "\n",
        "We have gone through and learnt how can we manually do the gradients in our networks, it's just some line of code for eact step and pretty simple (but not for Batch Norm formula). And I'll also back with another post to calculate how the grad of cross entropy is coming.\n",
        "\n",
        "In next lesson we will build RNN, LSTM, GRU, etc. Interesting and happly leanring!\n",
        "\n",
        "# resources\n",
        "\n",
        "1.  Notebook: <https://github.com/karpathy/nn-zero-to-hero/blob/master/lectures/makemore/makemore_part4_backprop.ipynb>;\n",
        "2.  Colab notebook: <https://colab.research.google.com/drive/1WV2oi2fh9XXyldh02wupFQX0wh5ZC-z-?usp=sharing>"
      ],
      "id": "74c69096"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}