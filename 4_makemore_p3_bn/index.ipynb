{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"NN-Z2H Lesson 4: Building makemore part 3 - Activations & Gradients, BatchNorm\"\n",
        "description: \"dive into the internals of MLPs, scrutinize the statistics of the forward pass activations, backward pass gradients, understand the health of your deep network, introduce batch normalization\"\n",
        "author:\n",
        "  - name: \"Tuan Le Khac\"\n",
        "    url: https://lktuan.github.io/\n",
        "categories: [til, python, andrej karpathy, nn-z2h, neural networks] \n",
        "date: 11-26-2024\n",
        "date-modified: 11-29-2024\n",
        "image: resnet50.png\n",
        "draft: false\n",
        "fig-cap-location: bottom\n",
        "editor: visual\n",
        "format:\n",
        "  html:\n",
        "    code-overflow: wrap\n",
        "    code-tools: true\n",
        "    code-fold: show\n",
        "    code-annotations: hover\n",
        "---"
      ],
      "id": "828c0894"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "::: {.callout-important title=\"This is not orginal content!\"}\n",
        "This is my study notes / codes along with Andrej Karpathy's \"[Neural Networks: Zero to Hero](https://www.youtube.com/watch?v=VMj-3S1tku0&list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ)\" series.\n",
        ":::\n",
        "\n",
        "We want to stay a bit longer with the MLPs, to have more concrete intuitive of the **activations** in the neural nets and **gradients** that flowing backwards. It's good to learn about the development history of these architectures. Since Recurrent Neural Network (RNN), they are although very *expressive* but not easily *optimizable* with current gradient techniques we have so far. Let's get started!\n",
        "\n",
        "# Part 1: intro\n",
        "\n",
        "## starter code"
      ],
      "id": "e925d0c2"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "id": "5524f3e2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import pandas as pd\n",
        "\n",
        "url = \"https://raw.githubusercontent.com/karpathy/makemore/refs/heads/master/names.txt\"\n",
        "words = pd.read_csv(url, header=None).iloc[:, 0].tolist()\n",
        "words[:8]"
      ],
      "id": "77057e00",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "len(words)"
      ],
      "id": "b2d8b19a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# build the vocabulary of characters and mapping to/from integer\n",
        "chars = sorted(list(set(''.join(words))))\n",
        "stoi = {s:i+1 for i, s in enumerate(chars)}\n",
        "stoi['.'] = 0\n",
        "itos = {i: s for s, i in stoi.items()}\n",
        "vocab_size = len(itos)\n",
        "print(itos)\n",
        "print(vocab_size)"
      ],
      "id": "a282068a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "block_size = 3\n",
        "# build the dataset\n",
        "def buid_dataset(words):\n",
        "    X, Y = [], []\n",
        "\n",
        "    for w in words:\n",
        "        context = [0] * block_size\n",
        "        for ch in w + '.':\n",
        "            ix = stoi[ch]\n",
        "            X.append(context)\n",
        "            Y.append(ix)\n",
        "            context = context[1:] + [ix]\n",
        "\n",
        "    X = torch.tensor(X)\n",
        "    Y = torch.tensor(Y)\n",
        "    print(X.shape, Y.shape)\n",
        "    return X, Y\n",
        "\n",
        "import random\n",
        "random.seed(42)\n",
        "random.shuffle(words)\n",
        "n1 = int(0.8 * len(words))\n",
        "n2 = int(0.9 * len(words))\n",
        "\n",
        "Xtr, Ytr = buid_dataset(words[:n1])        # 80#\n",
        "Xdev, Ydev = buid_dataset(words[n1:n2])    # 10%\n",
        "Xte, Yte = buid_dataset(words[n2:])        # 10%"
      ],
      "id": "168f1440",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# MLP revisited\n",
        "n_emb = 10 # no of dimensions of the embedding space.\n",
        "n_hidden = 200 # size of the hidden - tanh layer\n",
        "\n",
        "# Lookup table - 10 dimensional space\n",
        "g = torch.Generator().manual_seed(2147483647) # for reproductivity\n",
        "C = torch.randn((vocab_size, n_emb),                  generator=g)\n",
        "\n",
        "# Layer 1 - tanh - 300 neurons\n",
        "W1 = torch.randn((block_size * n_emb, n_hidden),      generator=g)\n",
        "b1 = torch.randn(n_hidden,                            generator=g)\n",
        "\n",
        "# Layer 2 - softmax\n",
        "W2 = torch.randn((n_hidden, vocab_size),              generator=g)\n",
        "b2 = torch.randn(vocab_size,                          generator=g)\n",
        "\n",
        "# All params\n",
        "parameters = [C, W1, b1, W2, b2]\n",
        "print(\"No of params: \", sum(p.nelement() for p in parameters))\n",
        "\n",
        "# Pre-training\n",
        "for p in parameters:\n",
        "    p.requires_grad = True"
      ],
      "id": "8269493d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Optimization\n",
        "max_steps = 50_000 #200_000\n",
        "batch_size = 32\n",
        "\n",
        "# Stats holders\n",
        "lossi = []\n",
        "\n",
        "# Training on Xtr, Ytr\n",
        "for i in range(max_steps):\n",
        "\n",
        "    # minibatch construct      \n",
        "    ix = torch.randint(0, Xtr.shape[0], (batch_size,)) \n",
        "    Xb, Yb = Xtr[ix], Ytr[ix] # batch X, Y\n",
        "\n",
        "    # forward pass:\n",
        "    emb = C[Xb] # embed the characters into vectors   \n",
        "    emb_cat = emb.view(emb.shape[0], -1) # concatenate the vectors\n",
        "    h_pre_act = emb_cat @ W1 + b1 # hidden layer pre-activation\n",
        "    h = torch.tanh(h_pre_act) # hidden layer\n",
        "    logits = h @ W2 + b2 # output layer\n",
        "    loss = F.cross_entropy(logits, Yb) # loss function\n",
        "\n",
        "    # backward pass:\n",
        "    for p in parameters:\n",
        "        p.grad = None\n",
        "    loss.backward()\n",
        "\n",
        "    # update\n",
        "    lr = 0.1 if i <= max_steps / 2 else 0.01 # step learning rate decay\n",
        "    for p in parameters:\n",
        "        p.data += - lr * p.grad\n",
        "\n",
        "    # track stats\n",
        "    if i % 10000 == 0: # print once every while\n",
        "      print(f'{i:7d}/{max_steps:7d}: {loss.item():.4f}')\n",
        "    lossi.append(loss.log10().item())"
      ],
      "id": "8eb6dad8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "plt.plot(lossi)"
      ],
      "id": "5ac9e026",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "@torch.no_grad() # disables gradient tracking\n",
        "def split_loss(split: str):\n",
        "  x, y = {\n",
        "    'train': (Xtr, Ytr),\n",
        "    'val': (Xdev, Ydev),\n",
        "    'test': (Xte, Yte)\n",
        "  }[split]\n",
        "  emb = C[x] # (N, block_size, n_emb)\n",
        "  emb_cat = emb.view(emb.shape[0], -1) # concatenate into (N, block_size * n_emb)\n",
        "  h = torch.tanh(emb_cat @ W1 + b1) # (N, n_hidden)\n",
        "  logits = h @ W2 + b2 # (N, vocab_size)\n",
        "  loss = F.cross_entropy(logits, y) # loss function\n",
        "  print(split, loss.item())\n",
        "\n",
        "split_loss('train')\n",
        "split_loss('val')"
      ],
      "id": "f07203ca",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# sample from the model\n",
        "g = torch.Generator().manual_seed(2147483647 + 10)\n",
        "\n",
        "for _ in range(20):\n",
        "    \n",
        "    out = []\n",
        "    context = [0] * block_size # initialize with all ...\n",
        "    while True:\n",
        "      # forward pass the neural net\n",
        "      emb = C[torch.tensor([context])] # (1,block_size,n_embd)\n",
        "      h = torch.tanh(emb.view(1, -1) @ W1 + b1)\n",
        "      logits = h @ W2 + b2\n",
        "      probs = F.softmax(logits, dim=1)\n",
        "      # sample from the distribution\n",
        "      ix = torch.multinomial(probs, num_samples=1, generator=g).item()\n",
        "      # shift the context window and track the samples\n",
        "      context = context[1:] + [ix]\n",
        "      out.append(ix)\n",
        "      # if we sample the special '.' token, break\n",
        "      if ix == 0:\n",
        "        break\n",
        "    \n",
        "    print(''.join(itos[i] for i in out)) # decode and print the generated word"
      ],
      "id": "7dadd794",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Okay so now our network has multiple things wrong at the initialization, let's list down below. The final code will be presented in the end of part 1, with `# ðŸ‘ˆ` for lines that had been added / modified. The right code cell below re-initializes states at the beginning of network's parameter (in my notebook, it's rendered **linearly**!)."
      ],
      "id": "57ed2366"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| code-fold: true\n",
        "n_emb = 10 # no of dimensions of the embedding space.\n",
        "n_hidden = 200 # size of the hidden - tanh layer\n",
        "# Lookup table - 10 dimensional space\n",
        "g = torch.Generator().manual_seed(2147483647) # for reproductivity\n",
        "C = torch.randn((vocab_size, n_emb),                  generator=g)\n",
        "# Layer 1 - tanh - 300 neurons\n",
        "W1 = torch.randn((block_size * n_emb, n_hidden),      generator=g)\n",
        "b1 = torch.randn(n_hidden,                            generator=g)\n",
        "# Layer 2 - softmax\n",
        "W2 = torch.randn((n_hidden, vocab_size),              generator=g)\n",
        "b2 = torch.randn(vocab_size,                          generator=g)\n",
        "# All params\n",
        "parameters = [C, W1, b1, W2, b2]\n",
        "# Pre-training\n",
        "for p in parameters:\n",
        "    p.requires_grad = True\n",
        "# Optimization\n",
        "max_steps = 50_000 #200_000\n",
        "batch_size = 32\n",
        "# Training on Xtr, Ytr\n",
        "for i in range(max_steps):\n",
        "    # minibatch construct      \n",
        "    ix = torch.randint(0, Xtr.shape[0], (batch_size,)) \n",
        "    Xb, Yb = Xtr[ix], Ytr[ix] # batch X, Y\n",
        "    # forward pass:\n",
        "    emb = C[Xb] # embed the characters into vectors   \n",
        "    emb_cat = emb.view(emb.shape[0], -1) # concatenate the vectors\n",
        "    h_pre_act = emb_cat @ W1 + b1 # hidden layer pre-activation\n",
        "    h = torch.tanh(h_pre_act) # hidden layer\n",
        "    logits = h @ W2 + b2 # output layer\n",
        "    loss = F.cross_entropy(logits, Yb) # loss function\n",
        "\n",
        "    break"
      ],
      "id": "1b637052",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## fixing the initial loss\n",
        "\n",
        "We can see at the `step = 0`, the loss was `27` and after some `k`s training loops it decreased to `1` or `2`. It extremely high at the begining. In practice, we should give the network somehow the expectation we want when generating a character after some characters (`3`)."
      ],
      "id": "e061aa8d"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "loss.item()"
      ],
      "id": "03128645",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this case, without training yet, we expect all `27` characters' possibilities to be equal (`1 / 27.0`) \\~ **uniform distribution**, so the loss \\~ negative log likelihood would be:"
      ],
      "id": "c39abd87"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "- torch.tensor(1 / 27.0).log()"
      ],
      "id": "ad589c73",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "It's far lower than `27`, we say that the network is **confidently wrong**. Andrej demonstrated by another simple 5 elements tensor and showed that the loss is lowest when all elements are equal.\n",
        "\n",
        "We want the `logits` to be low entropy as possible (but not equal to `0`, which will be showed later), we added multipliers `0.01` to `W2`, and `0` to `b2`. We got the loss to be `3.xx` at the beginning."
      ],
      "id": "a8f383f0"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| eval: false\n",
        "# Layer 2 - softmax\n",
        "W2 = torch.randn((n_hidden, vocab_size),              generator=g) * 0.01\n",
        "b2 = torch.randn(vocab_size,                          generator=g) * 0"
      ],
      "id": "13ee1cba",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now re-train the model and we will notice the the `lossi` will not look like the *hookey stick* anymore! Morever the final loss on train set and dev set is better!\n",
        "\n",
        "## fixing the saturated `tanh`\n",
        "\n",
        "The `logits` are now okay, the next problem is about the `h` - the activations of the hidden states! It's hard to see but in the output of code cell below, there are too many values of `1` and `-1` in this tensor."
      ],
      "id": "5f2d92a6"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "h"
      ],
      "id": "998f4ac9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Recall that `tanh` is activation function that squashing arbitrary numbers to the range `[-1:1]`. Let's visualize the distribution of `h`."
      ],
      "id": "c0dced61"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "plt.hist(h.view(-1).tolist(), 50); # the \";\" removes the presenting of data in code-block's output"
      ],
      "id": "d16646ae",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Most of them were distributed to the extreme values `-1` and `1`. Now come to the `h_pre_act`, we can see a **flat-tails distribution** from `-15` to `15`."
      ],
      "id": "440c44e3"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "plt.hist(h_pre_act.view(-1).tolist(), 50);"
      ],
      "id": "0abe8133",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Looking back to how we implemented `tanh` in `micrograd` (which is mathematically the same with `PyTorch`), we're multiplying the forward node's gradient with `(1 - t**2)`, which `t` is local `tanh`. When `tanh` is near `-1` or `1`, this is close to `0`, we are **killing the gradients**. We are stopping the backpropagation through this `tanh` unit."
      ],
      "id": "1b91c172"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| eval: false\n",
        "...\n",
        "    def tanh(self):\n",
        "        x = self.data\n",
        "        t = (math.exp(2*x) - 1) / (math.exp(2*x) + 1)\n",
        "        out = Value(t, (self, ), 'tanh')\n",
        "\n",
        "        def _backward():\n",
        "            self.grad += (1 - t**2) * out.grad\n",
        "        out._backward = _backward\n",
        "        return out\n",
        "..."
      ],
      "id": "37710802",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "When the gradients become zero, the previous nodes' gradients will be **vanishing**. We call this **saturated `tanh`**, this leads to **dead neurons** \\~ always off and because the gradient is zero then they will never be turned on, and happens for other activations as well: `sigmoid`, `ReLU`, etc (but less significant on `Leaky ReLU` or `ELU`). The network is not learning!\n",
        "\n",
        "The same with `logits`, now we want `h` to be more near zero, we add multipliers to the `W1` and `b1`:"
      ],
      "id": "7ac57c8d"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| eval: false\n",
        "# Layer 1 - tanh - 300 neurons\n",
        "W1 = torch.randn((block_size * n_emb, n_hidden),      generator=g)  * 0.2\n",
        "b1 = torch.randn(n_hidden,                            generator=g) * 0.01 # keep a little bit entropy, \n",
        "# It's okay to initialize the b1 to zero but AK found emperically this will enhance the optimiaztion"
      ],
      "id": "e7793cd4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can see now less peak distribution of `h`:\n",
        "\n",
        "::: columns\n",
        "::: {.column width=\"50%\"}\n",
        "![`tanh`](tanh_0.2mult.png)\n",
        ":::\n",
        "\n",
        "::: {.column width=\"50%\"}\n",
        "![pre-activation `tanh`](pre_act_tanh_0.2mult.png)\n",
        ":::\n",
        ":::\n",
        "\n",
        "## calculating the init scale: â€œKaiming initâ€\n",
        "\n",
        "Now let's look to the number `0.02`, in practice no one will set it manually. Let's look into the example below to see how parameters of Gaussian Distribution of `y` differ from `x` when multiplying by `W`.\n",
        "\n",
        "The question is how we set the `W` to preserve the Gaussian Distribution of X. Emperical researches found out that the multiplier to `W` should be square root of the \"fan in\", in this case is `10^0.5`."
      ],
      "id": "e44d999d"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "x = torch.randn(1000, 10)\n",
        "W = torch.randn(10, 200)\n",
        "y = x @ W\n",
        "\n",
        "W1 = torch.randn(10, 200) / 10**0.5\n",
        "y1 = x @ W1\n",
        "print(x.mean(), x.std())\n",
        "print(y.mean(), y.std())\n",
        "print(y1.mean(), y1.std())\n",
        "plt.figure(figsize=(20,5))\n",
        "plt.subplot(131).set_title(\"Input X\")\n",
        "plt.hist(x.view(-1).tolist(), 50, density=True);\n",
        "plt.subplot(132).set_title(\"Initial output y, expanded by W\")\n",
        "plt.hist(y.view(-1).tolist(), 50, density=True);\n",
        "plt.subplot(133).set_title(\"y1, preserve the X's Gaussian Dist\")\n",
        "plt.hist(y1.view(-1).tolist(), 50, density=True);"
      ],
      "id": "8a4ed615",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Please investigate more here:\n",
        "\n",
        "1.  Kaiming et al. paper: <https://arxiv.org/abs/1502.01852>\n",
        "2.  Implementation in `Pytorch`: <https://pytorch.org/docs/stable/nn.init.html#torch.nn.init.kaiming_normal_>\n",
        "\n",
        "It's recommended in Kaiming paper to use a **gain** multiplier base on nonlinearity/activation function ([here](https://pytorch.org/docs/stable/nn.init.html#torch.nn.init.calculate_gain)), for `tanh` it's `5/3`. We endup modified the initialization of `W1` with:"
      ],
      "id": "ed1983c8"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| eval: false\n",
        "W1 = torch.randn((block_size * n_emb, n_hidden),      generator=g)  * (5/3) / ((block_size * n_emb)**0.5) # * 0.2"
      ],
      "id": "9d9ec879",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this case is roughly `0.3`, re-train and although the loss only improved so insignificant (because previously we set it to be `0.2` - very close), but we've parameterized this hyper-constant.\n",
        "\n",
        "## batch normalization\n",
        "\n",
        "As discussed before, we dont want the `h_pre_act` to be way too small (\\~is not doing anything) or too large (\\~saturated), we want it to just roughly follow the standardized Gaussian Distribution (ie. mean equal to 0, std equal to 1).\n",
        "\n",
        "We've done it at the initialization, *why don't we just normalize the hidden states to be unit Gaussian*? in batch normalization, this can be achieved by 4 steps, demonstrated with our case:"
      ],
      "id": "8f265b11"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| eval: false\n",
        "# 1. mini-batch mean\n",
        "hpa_mean = h_pre_act.mean(0, keepdim=True)\n",
        "# 2. mini-batch variance / standard deviation\n",
        "hpa_std = h_pre_act.std(0, keepdim=True)\n",
        "# 3. normalize\n",
        "h_pre_act = (h_pre_act - hpa_mean) / hpa_std\n",
        "# 4. scale and shift\n",
        "# multiply by a \"gain\" then \"shift\" it with a bias\n",
        "bngain = torch.ones((1, n_hidden))\n",
        "bnbias = torch.zeros((1, n_hidden))\n",
        "h_pre_act = bngain * h_pre_act + bnbias"
      ],
      "id": "ee4de165",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We modified our code accordingly and re-run the code, actually this time the model did not improve much. Because actually this is very **simple and shallow** neural network. We also notice that the training loop now is slower than before, because the calculation volumn is bigger. Batch Normalization also unexpectedly comes up with a side effect, the forward and backward pass of any input now also depend on the mini-batch, not just itself (because of `mean()`/`std()`). This effect is suprisingly a good thing and acts as a **regularizer**.\n",
        "\n",
        "There are also non-coupling regularizers such as: Linear Normalization, Layer Normalization, Group Normalization.\n",
        "\n",
        "One othering to consider is in the deployment/testing phase, we dont want to use the batch norm calculated by a mini-batch. Instead we want to use the mean and standard deviation from the whole training data set:"
      ],
      "id": "4564619e"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| eval: false\n",
        "# calibrate the batch norm after training\n",
        "\n",
        "with torch.no_grad():\n",
        "    # pass the training set through\n",
        "    emb = C[x_train]\n",
        "    embcat = emb.view(-1, emb.shape[1] * emb.shape[2])\n",
        "    hpreact = embcat @ W1 + b1\n",
        "    # measure the mean/std over the entire training set\n",
        "    bnmean = hpreact.mean(0, keepdim=True)\n",
        "    bnstd = hpreact.std(0, keepdim=True)"
      ],
      "id": "f3bccee1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Rather, we can also use the running mean and standard deviation as implemented below which will give close estimates. Remaining 2 notes on the BN are:\n",
        "\n",
        "1.  Dividing zeros: we add a $\\epsilon$ value to the **variance** to avoid. We do not include this here as it likely not to happen with out example;\n",
        "2.  The bias `b1` will be subtracting in BN calculation, we will notice the `b1.grad` will be zeros as it does not impact any other calculation. Thus when using the BN, for layer before like weight, we should remove the bias. The `bnbias` now will be incharge for biasing the distributions.\n",
        "\n",
        "## real example: `resnet50` walkthrough\n",
        "\n",
        "The code AK presented here: <https://github.com/pytorch/vision/blob/main/torchvision/models/resnet.py#L108>\n",
        "\n",
        "![The architecture of ResNet-50 model.](resnet50.png)\n",
        "\n",
        "## summary of the lecture\n",
        "\n",
        "Understand the activations (non-linearity) and gradients is crucial when training deep / large neural networks, in part 1 we have observed some issue and come up with many solutions:\n",
        "\n",
        "1.  Confidently wrong of network at init leads to hookey stick for loss in training loop: adding multipliers to `logits`'s weights and biases;\n",
        "2.  Flat-tails distribution or saturated `tanh`: Kaiming init;\n",
        "3.  Normalization of the hidden states: introduction to BN.\n",
        "\n",
        "Our final code in part 1 (un-fold to see), `# ðŸ‘ˆ` indicates a change:"
      ],
      "id": "936ac2f5"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| code-fold: true\n",
        "block_size = 3\n",
        "\n",
        "# MLP revisited\n",
        "n_emb = 10 # no of dimensions of the embedding space.\n",
        "n_hidden = 200 # size of the hidden - tanh layer\n",
        "\n",
        "# Lookup table - 10 dimensional space\n",
        "g = torch.Generator().manual_seed(2147483647) # for reproductivity\n",
        "C = torch.randn((vocab_size, n_emb),                  generator=g)\n",
        "\n",
        "# Layer 1 - tanh - 300 neurons\n",
        "W1 = torch.randn((block_size * n_emb, n_hidden),      generator=g) * (5/3) / ((block_size * n_emb)**0.5) # * 0.2       # ðŸ‘ˆ\n",
        "# b1 = torch.randn(n_hidden,                            generator=g) * 0.01       # ðŸ‘ˆ\n",
        "\n",
        "# Layer 2 - softmax\n",
        "W2 = torch.randn((n_hidden, vocab_size),              generator=g) * 0.01       # ðŸ‘ˆ\n",
        "b2 = torch.randn(vocab_size,                          generator=g) * 0          # ðŸ‘ˆ\n",
        "\n",
        "# Batch Normalization gain and bias\n",
        "bngain = torch.ones((1, n_hidden))                                              # ðŸ‘ˆ\n",
        "bnbias = torch.zeros((1, n_hidden))                                             # ðŸ‘ˆ\n",
        "\n",
        "# Add running mean/std\n",
        "bnmean_running = torch.zeros((1, n_hidden))                             # ðŸ‘ˆ\n",
        "bnstd_running = torch.ones((1, n_hidden))                               # ðŸ‘ˆ\n",
        "\n",
        "# All params (deleted b1)\n",
        "parameters = [C, W1, W2, b2, bngain, bnbias]                                # ðŸ‘ˆ\n",
        "print(\"No of params: \", sum(p.nelement() for p in parameters))\n",
        "\n",
        "# Pre-training\n",
        "for p in parameters:\n",
        "    p.requires_grad = True\n",
        "\n",
        "# Optimization\n",
        "max_steps = 50_000 #200_000\n",
        "batch_size = 32\n",
        "\n",
        "# Stats holders\n",
        "lossi = []\n",
        "\n",
        "# Training on Xtr, Ytr\n",
        "for i in range(max_steps):\n",
        "\n",
        "    # minibatch construct      \n",
        "    ix = torch.randint(0, Xtr.shape[0], (batch_size,)) \n",
        "    Xb, Yb = Xtr[ix], Ytr[ix] # batch X, Y\n",
        "\n",
        "    # forward pass:\n",
        "    emb = C[Xb] # embed the characters into vectors   \n",
        "    emb_cat = emb.view(emb.shape[0], -1) # concatenate the vectors\n",
        "    # Linear layer\n",
        "    h_pre_act = emb_cat @ W1 # + b1 # hidden layer pre-activation                               # ðŸ‘ˆ\n",
        "    # BatchNorm layer\n",
        "    bnmeani = h_pre_act.mean(0, keepdim=True)                                                   # ðŸ‘ˆ\n",
        "    bnstdi = h_pre_act.std(0, keepdim=True)                                                     # ðŸ‘ˆ\n",
        "    h_pre_act = bngain * ((h_pre_act - bnmeani) / bnstdi) + bnbias                              # ðŸ‘ˆ\n",
        "    # Updating running mean and std (this runs outside the training loop)\n",
        "    with torch.no_grad():                                                                       # ðŸ‘ˆ\n",
        "        bnmean_running = 0.999 * bnmean_running + 0.001 * bnmeani                               # ðŸ‘ˆ\n",
        "        bnstd_running = 0.999 * bnstd_running + 0.001 * bnstdi                                  # ðŸ‘ˆ\n",
        "    # Non-linearity\n",
        "    h = torch.tanh(h_pre_act) # hidden layer\n",
        "    logits = h @ W2 + b2 # output layer\n",
        "    loss = F.cross_entropy(logits, Yb) # loss function\n",
        "\n",
        "    # backward pass:\n",
        "    for p in parameters:\n",
        "        p.grad = None\n",
        "    loss.backward()\n",
        "\n",
        "    # update\n",
        "    lr = 0.1 if i <= max_steps / 2 else 0.01 # step learning rate decay\n",
        "    for p in parameters:\n",
        "        p.data += - lr * p.grad\n",
        "\n",
        "    # track stats\n",
        "    if i % 10000 == 0: # print once every while\n",
        "      print(f'{i:7d}/{max_steps:7d}: {loss.item():.4f}')\n",
        "    lossi.append(loss.log10().item())"
      ],
      "id": "b8cfbf87",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "plt.plot(lossi)"
      ],
      "id": "50cfbecf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "@torch.no_grad() # disables gradient tracking\n",
        "def split_loss(split: str):\n",
        "  x, y = {\n",
        "    'train': (Xtr, Ytr),\n",
        "    'val': (Xdev, Ydev),\n",
        "    'test': (Xte, Yte)\n",
        "  }[split]\n",
        "  emb = C[x]\n",
        "  emb_cat = emb.view(emb.shape[0], -1) \n",
        "  h_pre_act = emb_cat @ W1 + b1                                                                                         # ðŸ‘ˆ \n",
        "  # h_pre_act = bngain * ((h_pre_act - h_pre_act.mean(0, keepdim=True)) / h_pre_act.std(0, keepdim=True)) + bnbias      # ðŸ‘ˆ\n",
        "  # h_pre_act = bngain * ((h_pre_act - bnmean) / bnstd) + bnbias                                                        # ðŸ‘ˆ\n",
        "  h_pre_act = bngain * ((h_pre_act - bnmean_running) / bnstd_running) + bnbias                                          # ðŸ‘ˆ\n",
        "  h = torch.tanh(h_pre_act) \n",
        "  logits = h @ W2 + b2\n",
        "  loss = F.cross_entropy(logits, y)\n",
        "  print(split, loss.item())\n",
        "\n",
        "split_loss('train')\n",
        "split_loss('val')"
      ],
      "id": "8549f2ce",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### loss logs\n",
        "\n",
        "The numbers somehow are approximate, I don't know why my Thinkpad-E14 gave different results when running codes multiple times ðŸ˜‚.\n",
        "\n",
        "+--------------+------------------------------------------------------------+--------------------------+\n",
        "| Step         | What we did                                                | Loss we got (accum)      |\n",
        "+==============+============================================================+==========================+\n",
        "| 1            | original                                                   | train 2.1169614791870117 |\n",
        "|              |                                                            |                          |\n",
        "|              |                                                            | val 2.1623435020446777   |\n",
        "+--------------+------------------------------------------------------------+--------------------------+\n",
        "| 2            | fixed softmax confidently wrong                            | train 2.0666463375091553 |\n",
        "|              |                                                            |                          |\n",
        "|              |                                                            | val 2.1468191146850586   |\n",
        "+--------------+------------------------------------------------------------+--------------------------+\n",
        "| 3            | fixed `tanh` layer too saturated at init                   | train 2.033477544784546  |\n",
        "|              |                                                            |                          |\n",
        "|              |                                                            | val 2.115907907485962    |\n",
        "+--------------+------------------------------------------------------------+--------------------------+\n",
        "| 4            | used semi principle \"kaiming init\" instead of hacking init | train 2.038902997970581  |\n",
        "|              |                                                            |                          |\n",
        "|              |                                                            | val 2.1138899326324463   |\n",
        "+--------------+------------------------------------------------------------+--------------------------+\n",
        "| 5            | added batch norm layer                                     | train 2.0662825107574463 |\n",
        "|              |                                                            |                          |\n",
        "|              |                                                            | val 2.1201331615448      |\n",
        "+--------------+------------------------------------------------------------+--------------------------+\n",
        "\n",
        ": Loss logs {tbl-colwidths=\"\\[10,40,50\\]\" .striped .hover}\n",
        "\n",
        "# Part 2: PyTorch-ifying the code, and train a deeper network\n",
        "\n",
        "Below is PyTorch-ified code by Andrej, some comments inputted by me:"
      ],
      "id": "111d5bcd"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Let's train a deeper network\n",
        "# The classes we create here are the same API as nn.Module in PyTorch\n",
        "\n",
        "class Linear:\n",
        "    \"\"\"\n",
        "    Simplifying Pytorch Linear Layer: https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear\n",
        "    \"\"\"\n",
        "    def __init__(self, fan_in, fan_out, bias=True):\n",
        "        self.weight = torch.randn((fan_in, fan_out), generator=g) / fan_in**0.5\n",
        "        self.bias = torch.zeros(fan_out) if bias else None\n",
        "\n",
        "    def __call__(self, x):\n",
        "        self.out = x @ self.weight\n",
        "        if self.bias is not None:\n",
        "            self.out += self.bias\n",
        "        return self.out\n",
        "\n",
        "    def parameters(self):\n",
        "        return [self.weight] + ([] if self.bias is None else [self.bias])\n",
        "\n",
        "\n",
        "class BatchNorm1d:\n",
        "    \"\"\"\n",
        "    Simplifying Pytorch BatchNorm1D: https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm1d.html\n",
        "    \"\"\" \n",
        "    def __init__(self, dim, eps=1e-5, momentum=0.1):\n",
        "        self.eps = eps\n",
        "        self.momentum = momentum\n",
        "        self.training = True # to differentiate usage of class in training or evaluation (using running mean/std)\n",
        "        # parameters (trained with backprop)\n",
        "        self.gamma = torch.ones(dim) # gain\n",
        "        self.beta = torch.zeros(dim) # bias\n",
        "        # buffers (trained with a running 'momentum update')\n",
        "        self.running_mean = torch.zeros(dim)\n",
        "        self.running_var = torch.ones(dim)\n",
        "\n",
        "    def __call__(self, x):\n",
        "        # calculate the forward pass\n",
        "        if self.training:\n",
        "            xmean = x.mean(0, keepdim=True) # batch mean\n",
        "            xvar = x.var(0, keepdim=True) # batch variance, follow the paper exactly\n",
        "        else:\n",
        "            xmean = self.running_mean\n",
        "            xvar = self.running_var\n",
        "        xhat = (x - xmean) / torch.sqrt(xvar + self.eps) # normalize to unit variance\n",
        "        self.out = self.gamma * xhat + self.beta # to tracking and visualizing data later on, PyTorch does not have this\n",
        "        # update the buffers\n",
        "        if self.training:\n",
        "            with torch.no_grad():\n",
        "                self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * xmean\n",
        "                self.running_var = (1 - self.momentum) * self.running_var + self.momentum * xvar\n",
        "        return self.out\n",
        "\n",
        "    def parameters(self):\n",
        "        return [self.gamma, self.beta]\n",
        "\n",
        "class Tanh:\n",
        "    \"\"\"\n",
        "    Just calculate the Tanh, just PyTorch: https://pytorch.org/docs/stable/generated/torch.nn.Tanh.html\n",
        "    \"\"\"\n",
        "    def __call__(self, x):\n",
        "        self.out = torch.tanh(x)\n",
        "        return self.out\n",
        "    def parameters(self):\n",
        "        return []\n",
        "\n",
        "n_embd = 10 # the dimensionality of the character embedding vectors\n",
        "n_hidden = 100 # the number of neurons in the hidden layer of the MLP\n",
        "g = torch.Generator().manual_seed(2147483647) # for reproducibility\n",
        "\n",
        "C = torch.randn((vocab_size, n_embd),            generator=g)\n",
        "\n",
        "layers = [\n",
        "    Linear(n_embd * block_size, n_hidden), Tanh(),\n",
        "    Linear(           n_hidden, n_hidden), Tanh(),\n",
        "    Linear(           n_hidden, n_hidden), Tanh(),\n",
        "    Linear(           n_hidden, n_hidden), Tanh(),\n",
        "    Linear(           n_hidden, n_hidden), Tanh(),\n",
        "    Linear(           n_hidden, vocab_size),\n",
        "]\n",
        "\n",
        "with torch.no_grad():\n",
        "    # last layer: make less confident\n",
        "    layers[-1].weight *= 0.1\n",
        "    # all other layers: apply gain\n",
        "    for layer in layers[:-1]:\n",
        "        if isinstance(layer, Linear):\n",
        "            layer.weight *= 5/3\n",
        "\n",
        "parameters = [C] + [p for layer in layers for p in layer.parameters()]\n",
        "print(sum(p.nelement() for p in parameters)) # number of parameters in total\n",
        "for p in parameters:\n",
        "    p.requires_grad = True"
      ],
      "id": "652a49a1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# same optimization as last time\n",
        "max_steps = 200000\n",
        "batch_size = 32\n",
        "lossi = []\n",
        "ud = []\n",
        "\n",
        "for i in range(max_steps):\n",
        "  \n",
        "    # minibatch construct\n",
        "    ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n",
        "    Xb, Yb = Xtr[ix], Ytr[ix] # batch X,Y\n",
        "\n",
        "    # forward pass\n",
        "    emb = C[Xb] # embed the characters into vectors\n",
        "    x = emb.view(emb.shape[0], -1) # concatenate the vectors\n",
        "    for layer in layers:\n",
        "        x = layer(x)\n",
        "    loss = F.cross_entropy(x, Yb) # loss function\n",
        "\n",
        "    # backward pass\n",
        "    for layer in layers:\n",
        "        layer.out.retain_grad() # AFTER_DEBUG: would take out retain_graph\n",
        "    for p in parameters:\n",
        "        p.grad = None\n",
        "    loss.backward()\n",
        "\n",
        "    # update\n",
        "    lr = 0.1 if i < 150000 else 0.01 # step learning rate decay\n",
        "    for p in parameters:\n",
        "        p.data += -lr * p.grad\n",
        "\n",
        "    # track stats\n",
        "    if i % 10000 == 0: # print every once in a while\n",
        "        print(f'{i:7d}/{max_steps:7d}: {loss.item():.4f}')\n",
        "    lossi.append(loss.log10().item())\n",
        "    with torch.no_grad():\n",
        "        ud.append([((lr*p.grad).std() / p.data.std()).log10().item() for p in parameters])\n",
        "\n",
        "    break\n",
        "    # if i >= 1000:\n",
        "    #     break # AFTER_DEBUG: would take out obviously to run full optimization"
      ],
      "id": "91351721",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## viz #1: forward pass activations statistics"
      ],
      "id": "3552a00c"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# visualize histograms\n",
        "plt.figure(figsize=(11, 3)) # width and height of the plot\n",
        "legends = []\n",
        "for i, layer in enumerate(layers[:-1]): # note: exclude the output layer\n",
        "  if isinstance(layer, Tanh):\n",
        "    t = layer.out\n",
        "    print('layer %d (%10s): mean %+.2f, std %.2f, saturated: %.2f%%' % (i, layer.__class__.__name__, t.mean(), t.std(), (t.abs() > 0.97).float().mean()*100))\n",
        "    hy, hx = torch.histogram(t, density=True)\n",
        "    plt.plot(hx[:-1].detach(), hy.detach())\n",
        "    legends.append(f'layer {i} ({layer.__class__.__name__}')\n",
        "plt.legend(legends);\n",
        "plt.title('activation distribution')"
      ],
      "id": "e02d0084",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If we set the gain to `1`, the std is shrinking, and the saturation is coming to zeros, due to the first layer is pretty decent, but the next ones are shrinking to zero because of the `tanh()` - a squashing function.\n",
        "\n",
        "``` md\n",
        "layer 1 (      Tanh): mean -0.02, std 0.62, saturated: 3.50%\n",
        "layer 3 (      Tanh): mean -0.00, std 0.48, saturated: 0.03%\n",
        "layer 5 (      Tanh): mean +0.00, std 0.41, saturated: 0.06%\n",
        "layer 7 (      Tanh): mean +0.00, std 0.35, saturated: 0.00%\n",
        "layer 9 (      Tanh): mean -0.02, std 0.32, saturated: 0.00%\n",
        "Text(0.5, 1.0, 'activation distribution')\n",
        "```\n",
        "\n",
        "![If the gain is 1](viz1_gain1.png)\n",
        "\n",
        "But if we set the gain is far too high, let's say `3`, we can see the saturation is too high.\n",
        "\n",
        "``` md\n",
        "layer 1 (      Tanh): mean -0.03, std 0.85, saturated: 47.66%\n",
        "layer 3 (      Tanh): mean +0.00, std 0.84, saturated: 40.47%\n",
        "layer 5 (      Tanh): mean -0.01, std 0.84, saturated: 42.38%\n",
        "layer 7 (      Tanh): mean -0.01, std 0.84, saturated: 42.00%\n",
        "layer 9 (      Tanh): mean -0.03, std 0.84, saturated: 42.41%\n",
        "Text(0.5, 1.0, 'activation distribution')\n",
        "```\n",
        "\n",
        "![If the gain is 3](viz1_gain3.png)\n",
        "\n",
        "So `5/3` is a nice one, balancing the std and saturation.\n",
        "\n",
        "::: {.callout-note title=\"Why 5/3?\"}\n",
        "A comment in his video explains why `5/3` is recommended, it comes from the avg of $[\\tanh(x)]^2$ where $x$ is distributed as a Gaussian:\n",
        "\n",
        "$\\int_{-\\infty}^{\\infty} \\frac{[\\tanh(x)]^2 \\exp(-\\frac{x^2}{2})}{\\sqrt{2\\pi}} \\, dx \\approx 0.39$\n",
        "\n",
        "> The square root of this value is how much the `tanh` squeezes the variance of the incoming variable: 0.39 \\*\\* .5 \\~= 0.63 \\~= 3/5 (hence 5/3 is just an approximation of the exact gain).\n",
        ":::\n",
        "\n",
        "## viz #2: backward pass gradient statistics\n",
        "\n",
        "Similarly, we can do the same thing with gradients. With the setting of gain as `5/3`, the distribution of gradients through layers quite the same. Layer by layer, the value of gradients will be shrank close to zero, the distributions would be more and more peak, so the gain here will help expanding those distributions."
      ],
      "id": "7c6d8825"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# visualize histograms\n",
        "plt.figure(figsize=(11, 3)) # width and height of the plot\n",
        "legends = []\n",
        "for i, layer in enumerate(layers[:-1]): # note: exclude the output layer\n",
        "  if isinstance(layer, Tanh):\n",
        "    t = layer.out.grad\n",
        "    print('layer %d (%10s): mean %+f, std %e' % (i, layer.__class__.__name__, t.mean(), t.std()))\n",
        "    hy, hx = torch.histogram(t, density=True)\n",
        "    plt.plot(hx[:-1].detach(), hy.detach())\n",
        "    legends.append(f'layer {i} ({layer.__class__.__name__}')\n",
        "plt.legend(legends);\n",
        "plt.title('gradient distribution')"
      ],
      "id": "60145651",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## the fully linear case of no non-linearity\n",
        "\n",
        "Now imagine if we remove the `tanh` from all layers, the recommend gain now for Linear is `1`."
      ],
      "id": "f5c9e2e8"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| eval: false\n",
        "layers = [\n",
        "  Linear(n_embd * block_size, n_hidden), #Tanh(),\n",
        "  Linear(           n_hidden, n_hidden), #Tanh(),\n",
        "  Linear(           n_hidden, n_hidden), #Tanh(),\n",
        "  Linear(           n_hidden, n_hidden), #Tanh(),\n",
        "  Linear(           n_hidden, n_hidden), #Tanh(),\n",
        "  Linear(           n_hidden, vocab_size),\n",
        "]"
      ],
      "id": "4cbd0c8a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "But you'll end up getting a pure linear network. No matter of how many Linear Layers you stacked up, it just the combination of all layers to a massive linear function $y = xA^T + b$, which will greatly limit the capacity of the neural nets.\n",
        "\n",
        "## viz #3: parameter activation and gradient statistics\n",
        "\n",
        "We can also visualize the distribution of paramaters, here below only weight for simplicity (ignoring gamma, beta, etc...). We observed mean, std, and the grad to data ratio (to see how much the data will be updated).\n",
        "\n",
        "Problem for the last layer is shown in code output below, the weights on last layer are 10 times bigger than previous ones, and the grad to data ratio is too high.\n",
        "\n",
        "We can try run 1st 1000 training loops and this can be slight reduced, but since we are using a simple optimizer SGD rather than modern one like Adam, it is still problematic."
      ],
      "id": "ea4577f0"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# visualize histograms\n",
        "plt.figure(figsize=(11, 3)) # width and height of the plot\n",
        "legends = []\n",
        "for i,p in enumerate(parameters):\n",
        "  t = p.grad\n",
        "  if p.ndim == 2:\n",
        "    print('weight %10s | mean %+f | std %e | grad:data ratio %e' % (tuple(p.shape), t.mean(), t.std(), t.std() / p.std()))\n",
        "    hy, hx = torch.histogram(t, density=True)\n",
        "    plt.plot(hx[:-1].detach(), hy.detach())\n",
        "    legends.append(f'{i} {tuple(p.shape)}')\n",
        "plt.legend(legends)\n",
        "plt.title('weights gradient distribution');"
      ],
      "id": "c833dcf5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## viz #4: update data ratio over time\n",
        "\n",
        "The grad to data above ratio is at the end not really informative (only at one point in time), what matter is actual amount which we change the data in these tensors (over time). AK introduce a tracking list `ud` (update to data). This calculates the ratio between (std) of the grad to the data of parameters (and `log10()` for a nicer viz) **without context of gradient**."
      ],
      "id": "1e1f322d"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| eval: false\n",
        "plt.figure(figsize=(11, 3))\n",
        "legends = []\n",
        "for i,p in enumerate(parameters):\n",
        "  if p.ndim == 2:\n",
        "    plt.plot([ud[j][i] for j in range(len(ud))])\n",
        "    legends.append('param %d' % i)\n",
        "plt.plot([0, len(ud)], [-3, -3], 'k') # these ratios should be ~1e-3, indicate on plot\n",
        "plt.legend(legends);"
      ],
      "id": "96cfc4d4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Below is the visualization from data collected after 1000 training loops:\n",
        "\n",
        "![Viz 4 1000](viz4_1000times.png)\n",
        "\n",
        "Recall what we did to the last layer, avoiding over confidence, so the pink line looks different among others. In general, the learning process are good, if we change the learning rate to `0.0001`, the chart looks much worse.\n",
        "\n",
        "Below are viz 1 after 1000 training loops:\n",
        "\n",
        "![Viz 1 1000](viz1_1000times.png)\n",
        "\n",
        "and viz 2:\n",
        "\n",
        "![Viz 2 1000](viz2_1000times.png)\n",
        "\n",
        "and viz 3:\n",
        "\n",
        "![Viz 3 1000](viz3_1000times.png)\n",
        "\n",
        "Pretty decent till now. Let's bring the BatchNorm back.\n",
        "\n",
        "## bringing back batchnorm, looking at the visualizations\n",
        "\n",
        "We re-define the layers, and change `gamma` in last layer under no gradient instead of `weight`. We also dont want the \"manual normalization\" fan-in, and the gain `5/3` as well:"
      ],
      "id": "604e3bc9"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| eval: false\n",
        "layers = [\n",
        "  Linear(n_embd * block_size, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),\n",
        "  Linear(           n_hidden, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),\n",
        "  Linear(           n_hidden, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),\n",
        "  Linear(           n_hidden, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),\n",
        "  Linear(           n_hidden, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),\n",
        "  Linear(           n_hidden, vocab_size, bias=False), BatchNorm1d(vocab_size),\n",
        "]"
      ],
      "id": "c97dc0c7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## summary of the lecture for real this time\n",
        "\n",
        "1.  Intruduction of Batch Normalization - the 1st one of modern innovation to stablize Deep NN training;\n",
        "2.  PyTorch-ifying code;\n",
        "3.  Introduction to some diagnostic tools that we can use to verify the network is in good state dynamically.\n",
        "\n",
        "What he did not try to improve here is the loss of the network. It's now somehow bottleneck not by the Optimization, but by the Context Length he suspect.\n",
        "\n",
        "> Training Neural Network is like balancing a pencil on a finger.\n",
        "\n",
        "Final network architecture and training:"
      ],
      "id": "c1aee87a"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| code-fold: true\n",
        "# BatchNorm1D and Tanh are the same\n",
        "class Linear:\n",
        "    \"\"\"\n",
        "    Simplifying Pytorch Linear Layer: https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear\n",
        "    \"\"\"\n",
        "    def __init__(self, fan_in, fan_out, bias=True):\n",
        "        self.weight = torch.randn((fan_in, fan_out), generator=g) # / fan_in**0.5\n",
        "        self.bias = torch.zeros(fan_out) if bias else None\n",
        "\n",
        "    def __call__(self, x):\n",
        "        self.out = x @ self.weight\n",
        "        if self.bias is not None:\n",
        "            self.out += self.bias\n",
        "        return self.out\n",
        "\n",
        "    def parameters(self):\n",
        "        return [self.weight] + ([] if self.bias is None else [self.bias])\n",
        "\n",
        "n_embd = 10 # the dimensionality of the character embedding vectors\n",
        "n_hidden = 100 # the number of neurons in the hidden layer of the MLP\n",
        "g = torch.Generator().manual_seed(2147483647) # for reproducibility\n",
        "C = torch.randn((vocab_size, n_embd),            generator=g)\n",
        "layers = [\n",
        "  Linear(n_embd * block_size, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),\n",
        "  Linear(           n_hidden, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),\n",
        "  Linear(           n_hidden, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),\n",
        "  Linear(           n_hidden, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),\n",
        "  Linear(           n_hidden, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),\n",
        "  Linear(           n_hidden, vocab_size, bias=False), BatchNorm1d(vocab_size),\n",
        "]\n",
        "\n",
        "with torch.no_grad():\n",
        "    # last layer: make less confident\n",
        "    layers[-1].gamma *= 0.1\n",
        "    # all other layers: apply gain\n",
        "    for layer in layers[:-1]:\n",
        "        if isinstance(layer, Linear):\n",
        "            layer.weight *= 1.0 #5/3\n",
        "\n",
        "parameters = [C] + [p for layer in layers for p in layer.parameters()]\n",
        "print(sum(p.nelement() for p in parameters)) # number of parameters in total\n",
        "for p in parameters:\n",
        "    p.requires_grad = True\n",
        "\n",
        "# same optimization as last time\n",
        "max_steps = 200000\n",
        "batch_size = 32\n",
        "lossi = []\n",
        "ud = []\n",
        "\n",
        "for i in range(max_steps):\n",
        "  \n",
        "    # minibatch construct\n",
        "    ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n",
        "    Xb, Yb = Xtr[ix], Ytr[ix] # batch X,Y\n",
        "\n",
        "    # forward pass\n",
        "    emb = C[Xb] # embed the characters into vectors\n",
        "    x = emb.view(emb.shape[0], -1) # concatenate the vectors\n",
        "    for layer in layers:\n",
        "        x = layer(x)\n",
        "    loss = F.cross_entropy(x, Yb) # loss function\n",
        "\n",
        "    # backward pass\n",
        "    for layer in layers:\n",
        "        layer.out.retain_grad() # AFTER_DEBUG: would take out retain_graph\n",
        "    for p in parameters:\n",
        "        p.grad = None\n",
        "    loss.backward()\n",
        "\n",
        "    # update\n",
        "    lr = 0.1 if i < 150000 else 0.01 # step learning rate decay\n",
        "    for p in parameters:\n",
        "        p.data += -lr * p.grad\n",
        "\n",
        "    # track stats\n",
        "    if i % 10000 == 0: # print every once in a while\n",
        "        print(f'{i:7d}/{max_steps:7d}: {loss.item():.4f}')\n",
        "    lossi.append(loss.log10().item())\n",
        "    with torch.no_grad():\n",
        "        ud.append([((lr*p.grad).std() / p.data.std()).log10().item() for p in parameters])\n",
        "\n",
        "    # break\n",
        "    # if i >= 1000:\n",
        "    #     break # AFTER_DEBUG: would take out obviously to run full optimization"
      ],
      "id": "a633d6be",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Final visualization:\n",
        "\n",
        "Viz 1:\n",
        "\n",
        "![Viz 1 final](viz1_final.png)\n",
        "\n",
        "Viz 2:\n",
        "\n",
        "![Viz 2 final](viz2_final.png)\n",
        "\n",
        "Viz 3:\n",
        "\n",
        "![Viz 3 final](viz3_final.png)\n",
        "\n",
        "Viz 4:\n",
        "\n",
        "![Viz 4 final](viz4_final.png)\n",
        "\n",
        "The final loss on train/val:"
      ],
      "id": "f972d3f5"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| code-fold: true\n",
        "@torch.no_grad() # this decorator disables gradient tracking\n",
        "def split_loss(split):\n",
        "  x,y = {\n",
        "    'train': (Xtr, Ytr),\n",
        "    'val': (Xdev, Ydev),\n",
        "    'test': (Xte, Yte),\n",
        "  }[split]\n",
        "  emb = C[x] # (N, block_size, n_embd)\n",
        "  x = emb.view(emb.shape[0], -1) # concat into (N, block_size * n_embd)\n",
        "  for layer in layers:\n",
        "    x = layer(x)\n",
        "  loss = F.cross_entropy(x, y)\n",
        "  print(split, loss.item())\n",
        "\n",
        "# put layers into eval mode\n",
        "for layer in layers:\n",
        "  layer.training = False\n",
        "split_loss('train')\n",
        "split_loss('val')"
      ],
      "id": "ef2fcb84",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Sample from the model:"
      ],
      "id": "175c5b30"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| code-fold: true\n",
        "# sample from the model\n",
        "g = torch.Generator().manual_seed(2147483647 + 10)\n",
        "\n",
        "for _ in range(20):\n",
        "    \n",
        "    out = []\n",
        "    context = [0] * block_size # initialize with all ...\n",
        "    while True:\n",
        "      # forward pass the neural net\n",
        "      emb = C[torch.tensor([context])] # (1,block_size,n_embd)\n",
        "      x = emb.view(emb.shape[0], -1) # concatenate the vectors\n",
        "      for layer in layers:\n",
        "        x = layer(x)\n",
        "      logits = x\n",
        "      probs = F.softmax(logits, dim=1)\n",
        "      # sample from the distribution\n",
        "      ix = torch.multinomial(probs, num_samples=1, generator=g).item()\n",
        "      # shift the context window and track the samples\n",
        "      context = context[1:] + [ix]\n",
        "      out.append(ix)\n",
        "      # if we sample the special '.' token, break\n",
        "      if ix == 0:\n",
        "        break\n",
        "    \n",
        "    print(''.join(itos[i] for i in out)) # decode and print the generated word"
      ],
      "id": "415e8e61",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Happy learning!\n",
        "\n",
        "## Exercises:\n",
        "\n",
        "-   E01: I did not get around to seeing what happens when you initialize all weights and biases to zero. Try this and train the neural net. You might think either that 1) the network trains just fine or 2) the network doesn't train at all, but actually it is 3) the network trains but only partially, and achieves a pretty bad final performance. Inspect the gradients and activations to figure out what is happening and why the network is only partially training, and what part is being trained exactly.\n",
        "-   E02: BatchNorm, unlike other normalization layers like LayerNorm/GroupNorm etc. has the big advantage that after training, the batchnorm gamma/beta can be \"folded into\" the weights of the preceeding Linear layers, effectively erasing the need to forward it at test time. Set up a small 3-layer MLP with batchnorms, train the network, then \"fold\" the batchnorm gamma/beta into the preceeding Linear layer's W,b by creating a new W2, b2 and erasing the batch norm. Verify that this gives the same forward pass during inference. i.e. we see that the batchnorm is there just for stabilizing the training, and can be thrown out after training is done! pretty cool.\n",
        "\n",
        "# resources:\n",
        "\n",
        "1.  other people learn from AK like me: <https://bedirtapkan.com/posts/blog_posts/karpathy_3_makemore_activations/>; <https://skeptric.com/index.html#category=makemore> - a replicate (?) with more OOPs on another dataset;\n",
        "2.  some good papers recommended by Andrej:\n",
        "    -   \"Kaiming init\" paper: <https://arxiv.org/abs/1502.01852>;\n",
        "    -   BatchNorm paper: <https://arxiv.org/abs/1502.03167>;\n",
        "    -   Bengio et al. 2003 MLP language model paper (pdf): <https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf>;\n",
        "    -   Good paper illustrating some of the problems with batchnorm in practice: <https://arxiv.org/abs/2105.07576>.\n",
        "3.  Notebook: <https://github.com/karpathy/nn-zero-to-hero/blob/master/lectures/makemore/makemore_part3_bn.ipynb>"
      ],
      "id": "9c647b5b"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}